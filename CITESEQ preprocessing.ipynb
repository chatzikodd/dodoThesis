{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf912612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5plugin\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3b6c56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = \"train_cite_inputs.h5\"\n",
    "\n",
    "with h5py.File(filename,'r') as hdf:\n",
    "    ls = list(hdf.keys())\n",
    "#     print(\"list of datasets in this file : \\n\",ls)\n",
    "#     print(\"list of keys in dataset \",hdf.get('train_cite_inputs').keys())\n",
    "    \n",
    "#     print(hdf['train_cite_inputs']['axis0'])\n",
    "    axis0 = hdf.get('train_cite_inputs').get('axis0')\n",
    "    axis0_arr = np.array(axis0)\n",
    "    axis0_arr = np.char.decode(axis0_arr) \n",
    "#     print(axis0_arr) \n",
    "    \n",
    "#     print(hdf['train_cite_inputs']['axis1'])\n",
    "    axis1 = hdf.get('train_cite_inputs').get('axis1')\n",
    "    axis1_arr = np.array(axis1)\n",
    "    axis1_arr = np.char.decode(axis1_arr)\n",
    "#     print(axis1_arr)\n",
    "    \n",
    "#     print(hdf['train_cite_inputs']['block0_items'])\n",
    "    block0_items = hdf.get('train_cite_inputs').get('block0_items')\n",
    "    block0_items_arr = np.array(block0_items)\n",
    "#     print(block0_items_arr)\n",
    "    \n",
    "    \n",
    "#     print(hdf['train_cite_inputs']['block0_values'])\n",
    "    block0_values = hdf.get('train_cite_inputs').get('block0_values')\n",
    "    block0_values_arr = np.array(block0_values)\n",
    "#     print(block0_values_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "910ec3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_input = pd.DataFrame(columns=axis0_arr, data=block0_values_arr,index=axis1_arr)\n",
    "# print(df_train_input.head(101))\n",
    "# print(list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff023ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"train_cite_targets.h5\"\n",
    "\n",
    "with h5py.File(filename,'r') as hdf:\n",
    "    ls =list(hdf.keys())\n",
    "#     print(\"list of datasets in this file : \\n\",ls)\n",
    "#     print(\"list of keys in dataset \",hdf.get('train_cite_targets').keys())\n",
    "#     print(hdf['train_cite_targets']['axis0'])\n",
    "    \n",
    "    axis0 = hdf.get('train_cite_targets').get('axis0')\n",
    "    axis0_arr = np.array(axis0)\n",
    "    axis0_arr = np.char.decode(axis0_arr) \n",
    "#     print(axis0_arr) \n",
    "    \n",
    "#     print(hdf['train_cite_targets']['axis1'])\n",
    "    axis1 = hdf.get('train_cite_targets').get('axis1')\n",
    "    axis1_arr = np.array(axis1)\n",
    "    axis1_arr = np.char.decode(axis1_arr)\n",
    "#     print(axis1_arr)\n",
    "    \n",
    "#     print(hdf['train_cite_targets']['block0_items'])\n",
    "    block0_items = hdf.get('train_cite_targets').get('block0_items')\n",
    "    block0_items_arr = np.array(block0_items)\n",
    "#     print(block0_items_arr)\n",
    "    \n",
    "    \n",
    "#     print(hdf['train_cite_targets']['block0_values'])\n",
    "    block0_values = hdf.get('train_cite_targets').get('block0_values')\n",
    "    block0_values_arr = np.array(block0_values)\n",
    "#     print(block0_values_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4905ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_target = pd.DataFrame(columns=axis0_arr, data=block0_values_arr,index=axis1_arr)\n",
    "target_columns = list(df_train_target.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ac46a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70988 70988\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train_input), len(df_train_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e07fea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished preprocessing\n"
     ]
    }
   ],
   "source": [
    "sample_size = 50000\n",
    "################\n",
    "####  PREPROCESSING ΑΛΗΘΕΙΑΣ\n",
    "###############\n",
    "df_train_input = df_train_input.iloc[:sample_size]\n",
    "df_train_target = df_train_target.iloc[:sample_size]\n",
    "\n",
    "#numero uno leaderboard preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler,normalize\n",
    "\n",
    "# df_norm = StandardScaler().fit_transform(df_train_input.values)\n",
    "df_norm = pd.DataFrame(normalize(df_train_input,norm='l2',axis=1),index=df_train_input.index, columns=df_train_input.columns)\n",
    "df_median = df_norm.sub(df_norm.median(axis=0), axis=1)\n",
    "\n",
    "tsvd = TruncatedSVD(n_components=128)\n",
    "df_tsvd = pd.DataFrame(tsvd.fit_transform(df_median), index=df_median.index)\n",
    "print(\"finished preprocessing\")\n",
    "df_train_input = df_tsvd\n",
    "#order by index (cell_id)\n",
    "df_train_input.sort_index(axis=0,inplace=True)\n",
    "df_train_target.sort_index(axis=0,inplace=True)\n",
    "\n",
    "\n",
    "df_merged = pd.merge(df_train_input,df_train_target,left_index=True, right_index=True)\n",
    "df_merged.head(100)\n",
    "\n",
    "\n",
    "target_columns = df_train_target.columns.values\n",
    "# print(target_columns)\n",
    "# Separate the input and target columns\n",
    "X = df_merged.drop(target_columns, axis=1)\n",
    "y = df_merged[target_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6ef8104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting MLP Regressor...\n",
      "best params :  {'activation': 'tanh', 'alpha': 0.2, 'hidden_layer_sizes': (100, 100), 'max_iter': 500, 'solver': 'lbfgs'}\n",
      "best r2 score :  0.1276774082288487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "print(\"starting gridsearch for MLP Regressor...\")\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats  import pearsonr\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#param optimization\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,),(100,),(50,50), (100,100), (64, 128)],\n",
    "    'activation':['relu','logistic','tanh'],\n",
    "    'solver':['adam','lbfgs','sgd'],\n",
    "    'max_iter':[50,100,200,300,500],\n",
    "    'alpha':[0.1, 0.2, 0.25],\n",
    "}\n",
    "# Initialize the MLPRegressor\n",
    "# mlp_model = MLPRegressor(hidden_layer_sizes=(64, 128),activation='relu', solver='adam', learning_rate='adaptive', max_iter=500,\n",
    "#                         alpha=0.01, early_stopping=True,validation_fraction=0.2)\n",
    "mlp_model  = MLPRegressor()\n",
    "grid_search = GridSearchCV(mlp_model, param_grid, cv = 5,n_jobs=-1)\n",
    "# Train the models on the dataset\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"best params : \" , grid_search.best_params_)\n",
    "print(\"best r2 score : \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9be2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting MLP Regressor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:  1.026996666152159\n",
      "mse:  2.446220031372783\n",
      "Mean Correlation Loss:  0.5743312840233705\n"
     ]
    }
   ],
   "source": [
    "print(\"starting MLP Regressor...\")\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats  import pearsonr\n",
    "\n",
    "# Initialize the MLPRegressor with best params\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(100,100),activation='tanh', solver='lbfgs', max_iter=1000,\n",
    "                        alpha=0.2)\n",
    "\n",
    "# {'activation': 'tanh', 'alpha': 0.2, 'hidden_layer_sizes': (100, 100), 'max_iter': 500, 'solver': 'lbfgs'}\n",
    "# best r2 score :  0.1276774082288487\n",
    "\n",
    "# Train the models on the dataset\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target values using the trained models\n",
    "mlp_y_preds = mlp_model.predict(X_test)\n",
    "\n",
    "#get model accuracy\n",
    "# accuracy = accuracy_score(y_test, mlp_y_preds)\n",
    "mae = mean_absolute_error(y_test, mlp_y_preds)\n",
    "mse = mean_squared_error(y_test, mlp_y_preds)\n",
    "# print(\"accuracy: \", accuracy)\n",
    "print(\"mae: \", mae)\n",
    "print(\"mse: \", mse)\n",
    "\n",
    "#accuracy for multiple columns\n",
    "\n",
    "threshold = 0.1\n",
    "corr_loss = []\n",
    "# print(y_test.shape)\n",
    "# print(type(y_test))\n",
    "# print(y_test.iloc[:10, 1])\n",
    "# print(type(mlp_y_preds))\n",
    "# print(mlp_y_preds.shape)\n",
    "# print(mlp_y_preds[:10,1])\n",
    "for i in range(y_test.shape[1]):  \n",
    "    correlation_loss_i = 1 - pearsonr(y_test.iloc[:, i], mlp_y_preds[:,i])[0]\n",
    "    corr_loss.append(correlation_loss_i)\n",
    "print(\"Mean Correlation Loss: \", np.mean(corr_loss))\n",
    "# print(\"successfull preds: \", np.mean(percentage_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0acc85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting gridsearch for CatBoost Regressor...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(<catboost.core.Pool object at 0x000002D22E944160>, dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-481f8533b772>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Fit the grid search to the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Print the best parameters and accuracy score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[0mrefit_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m         \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \"\"\"\n\u001b[0;32m    355\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0m\u001b[0;32m    260\u001b[0m                             \" a valid collection.\" % x)\n\u001b[0;32m    261\u001b[0m         \u001b[1;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array array(<catboost.core.Pool object at 0x000002D22E944160>, dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "## catboost param searching\n",
    "\n",
    "print(\"starting gridsearch for CatBoost Regressor...\")\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "\n",
    "#declare data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Load data into a Pool object\n",
    "train_data = Pool(X_train, y_train)\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {\n",
    "    'iterations': [100, 500, 1000],\n",
    "    'depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'l2_leaf_reg': [1, 3, 5],\n",
    "    'border_count': [32, 64, 128]\n",
    "}\n",
    "\n",
    "# Create a CatBoost regressor\n",
    "regressor = CatBoostRegressor(*params)\n",
    "\n",
    "# Perform grid search using cross-validation\n",
    "grid_search = GridSearchCV(regressor, param_grid=param_grid, cv=4, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_data)\n",
    "\n",
    "# Print the best parameters and accuracy score\n",
    "print(\"Best parameters for catboost: \", grid_search.best_params_)\n",
    "print(\"with best negative mean squared error: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db6e558",
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "C:/Program Files (x86)/Go Agent/pipelines/BuildMaster/catboost.git/catboost/private/libs/target/data_providers.cpp:612: Currently only multi-regression, multilabel and survival objectives work with multidimensional target",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fe28aef2169d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Train the model on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Predict on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5728\u001b[0m             \u001b[0mCatBoostRegressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss_function'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5730\u001b[1;33m         return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n\u001b[0m\u001b[0;32m   5731\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5732\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2353\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_cout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_cerr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m             \u001b[0mplot_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Training plots'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2355\u001b[1;33m             self._train(\n\u001b[0m\u001b[0;32m   2356\u001b[0m                 \u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2357\u001b[0m                 \u001b[0mtrain_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eval_sets\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1758\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1759\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1760\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: C:/Program Files (x86)/Go Agent/pipelines/BuildMaster/catboost.git/catboost/private/libs/target/data_providers.cpp:612: Currently only multi-regression, multilabel and survival objectives work with multidimensional target"
     ]
    }
   ],
   "source": [
    "## run catboost with best params\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a CatBoost regressor with the best parameters\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "regressor = CatBoostRegressor(iterations=1000, depth=8, learning_rate=0.1, l2_leaf_reg=5, border_count=128)\n",
    "\n",
    "# Train the model on the training data\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, mlp_y_preds)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "# print(\"accuracy: \", accuracy)\n",
    "print(\"mae: \", mae)\n",
    "print(\"mse: \", mse)\n",
    "\n",
    "#accuracy for multiple columns\n",
    "\n",
    "threshold = 0.1\n",
    "corr_loss = []\n",
    "# print(y_test.shape)\n",
    "# print(type(y_test))\n",
    "# print(y_test.iloc[:10, 1])\n",
    "# print(type(mlp_y_preds))\n",
    "# print(mlp_y_preds.shape)\n",
    "# print(mlp_y_preds[:10,1])\n",
    "for i in range(y_test.shape[1]):  \n",
    "    correlation_loss_i = 1 - pearsonr(y_test.iloc[:, i], y_pred[:,i])[0]\n",
    "    corr_loss.append(correlation_loss_i)\n",
    "print(\"Mean Correlation Loss: \", np.mean(corr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c8f56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 21.8376473\ttotal: 3.99s\tremaining: 1h 6m 29s\n",
      "1:\tlearn: 21.3805771\ttotal: 7.56s\tremaining: 1h 2m 51s\n",
      "2:\tlearn: 20.9911736\ttotal: 11.2s\tremaining: 1h 2m 13s\n",
      "3:\tlearn: 20.6211939\ttotal: 14.9s\tremaining: 1h 2m\n",
      "4:\tlearn: 20.2827165\ttotal: 18.6s\tremaining: 1h 1m 42s\n",
      "5:\tlearn: 20.0445764\ttotal: 22.3s\tremaining: 1h 1m 26s\n",
      "6:\tlearn: 19.8112624\ttotal: 25.9s\tremaining: 1h 1m 18s\n",
      "7:\tlearn: 19.5672409\ttotal: 29.6s\tremaining: 1h 1m 8s\n",
      "8:\tlearn: 19.3560135\ttotal: 33.3s\tremaining: 1h 1m 11s\n",
      "9:\tlearn: 19.1688421\ttotal: 37s\tremaining: 1h 1m 4s\n",
      "10:\tlearn: 19.0145480\ttotal: 40.7s\tremaining: 1h 59s\n",
      "11:\tlearn: 18.8573291\ttotal: 44.4s\tremaining: 1h 51s\n",
      "12:\tlearn: 18.7008914\ttotal: 48s\tremaining: 1h 45s\n",
      "13:\tlearn: 18.5942065\ttotal: 51.7s\tremaining: 1h 39s\n",
      "14:\tlearn: 18.4883974\ttotal: 55.3s\tremaining: 1h 30s\n",
      "15:\tlearn: 18.3988180\ttotal: 58.9s\tremaining: 1h 23s\n",
      "16:\tlearn: 18.3102593\ttotal: 1m 2s\tremaining: 1h 32s\n",
      "17:\tlearn: 18.2464861\ttotal: 1m 6s\tremaining: 1h 31s\n",
      "18:\tlearn: 18.1752126\ttotal: 1m 10s\tremaining: 1h 46s\n",
      "19:\tlearn: 18.0898350\ttotal: 1m 14s\tremaining: 1h 43s\n",
      "20:\tlearn: 18.0280515\ttotal: 1m 18s\tremaining: 1h 40s\n",
      "21:\tlearn: 17.9628730\ttotal: 1m 21s\tremaining: 1h 38s\n",
      "22:\tlearn: 17.9157183\ttotal: 1m 25s\tremaining: 1h 33s\n",
      "23:\tlearn: 17.8610988\ttotal: 1m 29s\tremaining: 1h 31s\n",
      "24:\tlearn: 17.8092940\ttotal: 1m 33s\tremaining: 1h 29s\n",
      "25:\tlearn: 17.7494029\ttotal: 1m 36s\tremaining: 1h 31s\n",
      "26:\tlearn: 17.7045001\ttotal: 1m 40s\tremaining: 1h 30s\n",
      "27:\tlearn: 17.6517717\ttotal: 1m 44s\tremaining: 1h 25s\n",
      "28:\tlearn: 17.6098805\ttotal: 1m 48s\tremaining: 1h 22s\n",
      "29:\tlearn: 17.5604678\ttotal: 1m 51s\tremaining: 1h 20s\n",
      "30:\tlearn: 17.5176440\ttotal: 1m 55s\tremaining: 1h 22s\n",
      "31:\tlearn: 17.4752993\ttotal: 1m 59s\tremaining: 1h 20s\n",
      "32:\tlearn: 17.4352570\ttotal: 2m 3s\tremaining: 1h 17s\n",
      "33:\tlearn: 17.4016832\ttotal: 2m 7s\tremaining: 1h 15s\n",
      "34:\tlearn: 17.3713702\ttotal: 2m 10s\tremaining: 1h 11s\n",
      "35:\tlearn: 17.3461240\ttotal: 2m 14s\tremaining: 1h 9s\n",
      "36:\tlearn: 17.3076121\ttotal: 2m 18s\tremaining: 1h 9s\n",
      "37:\tlearn: 17.2769409\ttotal: 2m 22s\tremaining: 1h 2s\n",
      "38:\tlearn: 17.2452744\ttotal: 2m 25s\tremaining: 59m 57s\n",
      "39:\tlearn: 17.2181526\ttotal: 2m 29s\tremaining: 59m 52s\n",
      "40:\tlearn: 17.1889647\ttotal: 2m 33s\tremaining: 59m 48s\n",
      "41:\tlearn: 17.1598825\ttotal: 2m 37s\tremaining: 59m 44s\n",
      "42:\tlearn: 17.1407744\ttotal: 2m 40s\tremaining: 59m 39s\n",
      "43:\tlearn: 17.1096439\ttotal: 2m 44s\tremaining: 59m 34s\n",
      "44:\tlearn: 17.0864476\ttotal: 2m 48s\tremaining: 59m 27s\n",
      "45:\tlearn: 17.0608953\ttotal: 2m 51s\tremaining: 59m 21s\n",
      "46:\tlearn: 17.0330974\ttotal: 2m 55s\tremaining: 59m 15s\n",
      "47:\tlearn: 17.0109239\ttotal: 2m 58s\tremaining: 59m 9s\n",
      "48:\tlearn: 16.9967857\ttotal: 3m 2s\tremaining: 59m 2s\n",
      "49:\tlearn: 16.9684967\ttotal: 3m 6s\tremaining: 58m 58s\n",
      "50:\tlearn: 16.9525381\ttotal: 3m 9s\tremaining: 58m 51s\n",
      "51:\tlearn: 16.9352177\ttotal: 3m 13s\tremaining: 58m 45s\n",
      "52:\tlearn: 16.9070073\ttotal: 3m 17s\tremaining: 58m 42s\n",
      "53:\tlearn: 16.8931532\ttotal: 3m 20s\tremaining: 58m 37s\n",
      "54:\tlearn: 16.8682500\ttotal: 3m 24s\tremaining: 58m 31s\n",
      "55:\tlearn: 16.8539996\ttotal: 3m 27s\tremaining: 58m 25s\n",
      "56:\tlearn: 16.8299945\ttotal: 3m 31s\tremaining: 58m 20s\n",
      "57:\tlearn: 16.8098195\ttotal: 3m 35s\tremaining: 58m 16s\n",
      "58:\tlearn: 16.7777128\ttotal: 3m 39s\tremaining: 58m 13s\n",
      "59:\tlearn: 16.7565586\ttotal: 3m 42s\tremaining: 58m 10s\n",
      "60:\tlearn: 16.7389004\ttotal: 3m 46s\tremaining: 58m 4s\n",
      "61:\tlearn: 16.7238293\ttotal: 3m 50s\tremaining: 57m 59s\n",
      "62:\tlearn: 16.7040052\ttotal: 3m 53s\tremaining: 57m 56s\n",
      "63:\tlearn: 16.6879062\ttotal: 3m 57s\tremaining: 57m 51s\n",
      "64:\tlearn: 16.6716287\ttotal: 4m 1s\tremaining: 57m 49s\n",
      "65:\tlearn: 16.6531812\ttotal: 4m 4s\tremaining: 57m 46s\n",
      "66:\tlearn: 16.6402152\ttotal: 4m 8s\tremaining: 57m 41s\n",
      "67:\tlearn: 16.6130811\ttotal: 4m 12s\tremaining: 57m 37s\n",
      "68:\tlearn: 16.5990846\ttotal: 4m 15s\tremaining: 57m 33s\n",
      "69:\tlearn: 16.5789037\ttotal: 4m 19s\tremaining: 57m 29s\n",
      "70:\tlearn: 16.5647304\ttotal: 4m 23s\tremaining: 57m 24s\n",
      "71:\tlearn: 16.5395622\ttotal: 4m 27s\tremaining: 57m 21s\n",
      "72:\tlearn: 16.5221408\ttotal: 4m 30s\tremaining: 57m 16s\n",
      "73:\tlearn: 16.5032448\ttotal: 4m 34s\tremaining: 57m 14s\n",
      "74:\tlearn: 16.4854174\ttotal: 4m 38s\tremaining: 57m 10s\n",
      "75:\tlearn: 16.4688052\ttotal: 4m 41s\tremaining: 57m 6s\n",
      "76:\tlearn: 16.4540050\ttotal: 4m 45s\tremaining: 57m 3s\n",
      "77:\tlearn: 16.4379646\ttotal: 4m 49s\tremaining: 56m 59s\n",
      "78:\tlearn: 16.4221645\ttotal: 4m 53s\tremaining: 56m 55s\n",
      "79:\tlearn: 16.4021892\ttotal: 4m 56s\tremaining: 56m 52s\n",
      "80:\tlearn: 16.3886563\ttotal: 5m\tremaining: 56m 48s\n",
      "81:\tlearn: 16.3700854\ttotal: 5m 4s\tremaining: 56m 46s\n",
      "82:\tlearn: 16.3552038\ttotal: 5m 8s\tremaining: 56m 44s\n",
      "83:\tlearn: 16.3432572\ttotal: 5m 11s\tremaining: 56m 41s\n",
      "84:\tlearn: 16.3348541\ttotal: 5m 15s\tremaining: 56m 39s\n",
      "85:\tlearn: 16.3220659\ttotal: 5m 19s\tremaining: 56m 36s\n",
      "86:\tlearn: 16.3067612\ttotal: 5m 23s\tremaining: 56m 32s\n",
      "87:\tlearn: 16.2930680\ttotal: 5m 27s\tremaining: 56m 29s\n",
      "88:\tlearn: 16.2793327\ttotal: 5m 30s\tremaining: 56m 25s\n",
      "89:\tlearn: 16.2597990\ttotal: 5m 34s\tremaining: 56m 22s\n",
      "90:\tlearn: 16.2422241\ttotal: 5m 38s\tremaining: 56m 19s\n",
      "91:\tlearn: 16.2281536\ttotal: 5m 42s\tremaining: 56m 16s\n",
      "92:\tlearn: 16.2125576\ttotal: 5m 45s\tremaining: 56m 13s\n",
      "93:\tlearn: 16.2004249\ttotal: 5m 49s\tremaining: 56m 9s\n",
      "94:\tlearn: 16.1888412\ttotal: 5m 53s\tremaining: 56m 5s\n",
      "95:\tlearn: 16.1744961\ttotal: 5m 57s\tremaining: 56m 1s\n",
      "96:\tlearn: 16.1599024\ttotal: 6m\tremaining: 55m 57s\n",
      "97:\tlearn: 16.1404171\ttotal: 6m 4s\tremaining: 55m 54s\n",
      "98:\tlearn: 16.1315298\ttotal: 6m 8s\tremaining: 55m 49s\n",
      "99:\tlearn: 16.1230994\ttotal: 6m 11s\tremaining: 55m 44s\n",
      "100:\tlearn: 16.1109065\ttotal: 6m 15s\tremaining: 55m 40s\n",
      "101:\tlearn: 16.0999082\ttotal: 6m 19s\tremaining: 55m 37s\n",
      "102:\tlearn: 16.0900969\ttotal: 6m 22s\tremaining: 55m 33s\n",
      "103:\tlearn: 16.0797542\ttotal: 6m 26s\tremaining: 55m 29s\n",
      "104:\tlearn: 16.0699726\ttotal: 6m 30s\tremaining: 55m 28s\n",
      "105:\tlearn: 16.0501236\ttotal: 6m 34s\tremaining: 55m 27s\n",
      "106:\tlearn: 16.0402794\ttotal: 6m 38s\tremaining: 55m 22s\n",
      "107:\tlearn: 16.0287363\ttotal: 6m 41s\tremaining: 55m 18s\n",
      "108:\tlearn: 16.0149431\ttotal: 6m 45s\tremaining: 55m 14s\n",
      "109:\tlearn: 16.0019587\ttotal: 6m 49s\tremaining: 55m 10s\n",
      "110:\tlearn: 15.9932074\ttotal: 6m 52s\tremaining: 55m 5s\n",
      "111:\tlearn: 15.9782731\ttotal: 6m 56s\tremaining: 55m 1s\n",
      "112:\tlearn: 15.9697620\ttotal: 7m\tremaining: 54m 56s\n",
      "113:\tlearn: 15.9526713\ttotal: 7m 3s\tremaining: 54m 53s\n",
      "114:\tlearn: 15.9413249\ttotal: 7m 7s\tremaining: 54m 49s\n",
      "115:\tlearn: 15.9265790\ttotal: 7m 11s\tremaining: 54m 44s\n",
      "116:\tlearn: 15.9136635\ttotal: 7m 14s\tremaining: 54m 40s\n",
      "117:\tlearn: 15.9051018\ttotal: 7m 18s\tremaining: 54m 36s\n",
      "118:\tlearn: 15.8879901\ttotal: 7m 22s\tremaining: 54m 32s\n",
      "119:\tlearn: 15.8733893\ttotal: 7m 25s\tremaining: 54m 29s\n",
      "120:\tlearn: 15.8638818\ttotal: 7m 29s\tremaining: 54m 24s\n",
      "121:\tlearn: 15.8541529\ttotal: 7m 33s\tremaining: 54m 20s\n",
      "122:\tlearn: 15.8446438\ttotal: 7m 36s\tremaining: 54m 16s\n",
      "123:\tlearn: 15.8310630\ttotal: 7m 40s\tremaining: 54m 12s\n",
      "124:\tlearn: 15.8214526\ttotal: 7m 44s\tremaining: 54m 8s\n",
      "125:\tlearn: 15.8091866\ttotal: 7m 47s\tremaining: 54m 4s\n",
      "126:\tlearn: 15.7977430\ttotal: 7m 51s\tremaining: 54m\n",
      "127:\tlearn: 15.7846885\ttotal: 7m 55s\tremaining: 53m 56s\n",
      "128:\tlearn: 15.7748970\ttotal: 7m 58s\tremaining: 53m 52s\n",
      "129:\tlearn: 15.7648661\ttotal: 8m 2s\tremaining: 53m 48s\n",
      "130:\tlearn: 15.7532543\ttotal: 8m 6s\tremaining: 53m 44s\n",
      "131:\tlearn: 15.7441036\ttotal: 8m 9s\tremaining: 53m 39s\n",
      "132:\tlearn: 15.7346981\ttotal: 8m 13s\tremaining: 53m 35s\n",
      "133:\tlearn: 15.7278643\ttotal: 8m 16s\tremaining: 53m 31s\n",
      "134:\tlearn: 15.7183905\ttotal: 8m 20s\tremaining: 53m 26s\n",
      "135:\tlearn: 15.7125706\ttotal: 8m 24s\tremaining: 53m 22s\n",
      "136:\tlearn: 15.7033627\ttotal: 8m 27s\tremaining: 53m 18s\n",
      "137:\tlearn: 15.6948382\ttotal: 8m 31s\tremaining: 53m 13s\n",
      "138:\tlearn: 15.6819679\ttotal: 8m 35s\tremaining: 53m 10s\n",
      "139:\tlearn: 15.6702774\ttotal: 8m 38s\tremaining: 53m 6s\n",
      "140:\tlearn: 15.6597351\ttotal: 8m 42s\tremaining: 53m 2s\n",
      "141:\tlearn: 15.6531782\ttotal: 8m 46s\tremaining: 52m 58s\n",
      "142:\tlearn: 15.6383498\ttotal: 8m 49s\tremaining: 52m 55s\n",
      "143:\tlearn: 15.6299732\ttotal: 8m 53s\tremaining: 52m 52s\n",
      "144:\tlearn: 15.6180774\ttotal: 8m 57s\tremaining: 52m 49s\n",
      "145:\tlearn: 15.6088874\ttotal: 9m 1s\tremaining: 52m 45s\n",
      "146:\tlearn: 15.5947706\ttotal: 9m 4s\tremaining: 52m 42s\n",
      "147:\tlearn: 15.5879554\ttotal: 9m 8s\tremaining: 52m 38s\n",
      "148:\tlearn: 15.5796317\ttotal: 9m 12s\tremaining: 52m 34s\n",
      "149:\tlearn: 15.5709972\ttotal: 9m 16s\tremaining: 52m 30s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150:\tlearn: 15.5654539\ttotal: 9m 19s\tremaining: 52m 26s\n",
      "151:\tlearn: 15.5579388\ttotal: 9m 23s\tremaining: 52m 23s\n",
      "152:\tlearn: 15.5471315\ttotal: 9m 27s\tremaining: 52m 21s\n",
      "153:\tlearn: 15.5360727\ttotal: 9m 31s\tremaining: 52m 18s\n",
      "154:\tlearn: 15.5260925\ttotal: 9m 35s\tremaining: 52m 14s\n",
      "155:\tlearn: 15.5165562\ttotal: 9m 38s\tremaining: 52m 11s\n",
      "156:\tlearn: 15.5093487\ttotal: 9m 42s\tremaining: 52m 8s\n",
      "157:\tlearn: 15.5036040\ttotal: 9m 46s\tremaining: 52m 4s\n",
      "158:\tlearn: 15.4964520\ttotal: 9m 49s\tremaining: 52m\n",
      "159:\tlearn: 15.4912450\ttotal: 9m 53s\tremaining: 51m 55s\n",
      "160:\tlearn: 15.4831595\ttotal: 9m 57s\tremaining: 51m 51s\n",
      "161:\tlearn: 15.4709713\ttotal: 10m\tremaining: 51m 47s\n",
      "162:\tlearn: 15.4592319\ttotal: 10m 4s\tremaining: 51m 43s\n",
      "163:\tlearn: 15.4530417\ttotal: 10m 8s\tremaining: 51m 39s\n",
      "164:\tlearn: 15.4421962\ttotal: 10m 11s\tremaining: 51m 35s\n",
      "165:\tlearn: 15.4337887\ttotal: 10m 15s\tremaining: 51m 31s\n",
      "166:\tlearn: 15.4240965\ttotal: 10m 19s\tremaining: 51m 28s\n",
      "167:\tlearn: 15.4161948\ttotal: 10m 22s\tremaining: 51m 24s\n",
      "168:\tlearn: 15.4061536\ttotal: 10m 26s\tremaining: 51m 20s\n",
      "169:\tlearn: 15.3991703\ttotal: 10m 30s\tremaining: 51m 16s\n",
      "170:\tlearn: 15.3909205\ttotal: 10m 34s\tremaining: 51m 13s\n",
      "171:\tlearn: 15.3821716\ttotal: 10m 38s\tremaining: 51m 13s\n",
      "172:\tlearn: 15.3752184\ttotal: 10m 43s\tremaining: 51m 13s\n",
      "173:\tlearn: 15.3674228\ttotal: 10m 47s\tremaining: 51m 12s\n",
      "174:\tlearn: 15.3635457\ttotal: 10m 50s\tremaining: 51m 8s\n",
      "175:\tlearn: 15.3550436\ttotal: 10m 54s\tremaining: 51m 4s\n",
      "176:\tlearn: 15.3467274\ttotal: 10m 58s\tremaining: 51m 1s\n",
      "177:\tlearn: 15.3392127\ttotal: 11m 2s\tremaining: 50m 57s\n",
      "178:\tlearn: 15.3351126\ttotal: 11m 5s\tremaining: 50m 54s\n",
      "179:\tlearn: 15.3317702\ttotal: 11m 10s\tremaining: 50m 52s\n",
      "180:\tlearn: 15.3237164\ttotal: 11m 14s\tremaining: 50m 53s\n",
      "181:\tlearn: 15.3154859\ttotal: 11m 18s\tremaining: 50m 50s\n",
      "182:\tlearn: 15.3081912\ttotal: 11m 23s\tremaining: 50m 50s\n",
      "183:\tlearn: 15.3000627\ttotal: 11m 27s\tremaining: 50m 49s\n",
      "184:\tlearn: 15.2904439\ttotal: 11m 32s\tremaining: 50m 50s\n",
      "185:\tlearn: 15.2830157\ttotal: 11m 36s\tremaining: 50m 49s\n",
      "186:\tlearn: 15.2783872\ttotal: 11m 40s\tremaining: 50m 46s\n",
      "187:\tlearn: 15.2704580\ttotal: 11m 44s\tremaining: 50m 42s\n",
      "188:\tlearn: 15.2648387\ttotal: 11m 48s\tremaining: 50m 38s\n",
      "189:\tlearn: 15.2574358\ttotal: 11m 51s\tremaining: 50m 34s\n",
      "190:\tlearn: 15.2424946\ttotal: 11m 55s\tremaining: 50m 31s\n",
      "191:\tlearn: 15.2332342\ttotal: 11m 59s\tremaining: 50m 27s\n",
      "192:\tlearn: 15.2261178\ttotal: 12m 3s\tremaining: 50m 24s\n",
      "193:\tlearn: 15.2225382\ttotal: 12m 6s\tremaining: 50m 20s\n",
      "194:\tlearn: 15.2169320\ttotal: 12m 10s\tremaining: 50m 15s\n",
      "195:\tlearn: 15.2094662\ttotal: 12m 14s\tremaining: 50m 11s\n",
      "196:\tlearn: 15.2022267\ttotal: 12m 17s\tremaining: 50m 7s\n",
      "197:\tlearn: 15.1967003\ttotal: 12m 21s\tremaining: 50m 3s\n",
      "198:\tlearn: 15.1849573\ttotal: 12m 25s\tremaining: 49m 59s\n",
      "199:\tlearn: 15.1790314\ttotal: 12m 28s\tremaining: 49m 55s\n",
      "200:\tlearn: 15.1682172\ttotal: 12m 32s\tremaining: 49m 51s\n",
      "201:\tlearn: 15.1625569\ttotal: 12m 36s\tremaining: 49m 47s\n",
      "202:\tlearn: 15.1551498\ttotal: 12m 39s\tremaining: 49m 43s\n",
      "203:\tlearn: 15.1489324\ttotal: 12m 43s\tremaining: 49m 39s\n",
      "204:\tlearn: 15.1448086\ttotal: 12m 47s\tremaining: 49m 35s\n",
      "205:\tlearn: 15.1403577\ttotal: 12m 50s\tremaining: 49m 30s\n",
      "206:\tlearn: 15.1330782\ttotal: 12m 54s\tremaining: 49m 27s\n",
      "207:\tlearn: 15.1208658\ttotal: 12m 58s\tremaining: 49m 23s\n",
      "208:\tlearn: 15.1138225\ttotal: 13m 2s\tremaining: 49m 19s\n",
      "209:\tlearn: 15.1059402\ttotal: 13m 5s\tremaining: 49m 15s\n",
      "210:\tlearn: 15.0955335\ttotal: 13m 9s\tremaining: 49m 11s\n",
      "211:\tlearn: 15.0868504\ttotal: 13m 13s\tremaining: 49m 8s\n",
      "212:\tlearn: 15.0791590\ttotal: 13m 16s\tremaining: 49m 4s\n",
      "213:\tlearn: 15.0716295\ttotal: 13m 20s\tremaining: 49m\n",
      "214:\tlearn: 15.0657728\ttotal: 13m 24s\tremaining: 48m 56s\n",
      "215:\tlearn: 15.0575393\ttotal: 13m 27s\tremaining: 48m 52s\n",
      "216:\tlearn: 15.0523763\ttotal: 13m 31s\tremaining: 48m 48s\n",
      "217:\tlearn: 15.0450390\ttotal: 13m 35s\tremaining: 48m 44s\n",
      "218:\tlearn: 15.0362005\ttotal: 13m 38s\tremaining: 48m 40s\n",
      "219:\tlearn: 15.0292553\ttotal: 13m 42s\tremaining: 48m 36s\n",
      "220:\tlearn: 15.0228179\ttotal: 13m 46s\tremaining: 48m 31s\n",
      "221:\tlearn: 15.0123553\ttotal: 13m 49s\tremaining: 48m 28s\n",
      "222:\tlearn: 15.0020669\ttotal: 13m 53s\tremaining: 48m 25s\n",
      "223:\tlearn: 14.9965870\ttotal: 13m 57s\tremaining: 48m 21s\n",
      "224:\tlearn: 14.9903967\ttotal: 14m 1s\tremaining: 48m 17s\n",
      "225:\tlearn: 14.9864056\ttotal: 14m 4s\tremaining: 48m 13s\n",
      "226:\tlearn: 14.9819741\ttotal: 14m 8s\tremaining: 48m 8s\n",
      "227:\tlearn: 14.9710844\ttotal: 14m 12s\tremaining: 48m 5s\n",
      "228:\tlearn: 14.9629602\ttotal: 14m 15s\tremaining: 48m 1s\n",
      "229:\tlearn: 14.9553664\ttotal: 14m 19s\tremaining: 47m 57s\n",
      "230:\tlearn: 14.9471168\ttotal: 14m 23s\tremaining: 47m 52s\n",
      "231:\tlearn: 14.9392839\ttotal: 14m 26s\tremaining: 47m 49s\n",
      "232:\tlearn: 14.9333484\ttotal: 14m 30s\tremaining: 47m 44s\n",
      "233:\tlearn: 14.9240654\ttotal: 14m 33s\tremaining: 47m 40s\n",
      "234:\tlearn: 14.9165652\ttotal: 14m 37s\tremaining: 47m 37s\n",
      "235:\tlearn: 14.9069163\ttotal: 14m 41s\tremaining: 47m 33s\n",
      "236:\tlearn: 14.8997787\ttotal: 14m 44s\tremaining: 47m 28s\n",
      "237:\tlearn: 14.8944371\ttotal: 14m 48s\tremaining: 47m 24s\n",
      "238:\tlearn: 14.8863375\ttotal: 14m 52s\tremaining: 47m 20s\n",
      "239:\tlearn: 14.8833249\ttotal: 14m 55s\tremaining: 47m 16s\n",
      "240:\tlearn: 14.8747898\ttotal: 14m 59s\tremaining: 47m 12s\n",
      "241:\tlearn: 14.8706509\ttotal: 15m 2s\tremaining: 47m 8s\n",
      "242:\tlearn: 14.8645113\ttotal: 15m 6s\tremaining: 47m 3s\n",
      "243:\tlearn: 14.8581627\ttotal: 15m 10s\tremaining: 46m 59s\n",
      "244:\tlearn: 14.8511270\ttotal: 15m 13s\tremaining: 46m 55s\n",
      "245:\tlearn: 14.8431723\ttotal: 15m 17s\tremaining: 46m 51s\n",
      "246:\tlearn: 14.8358552\ttotal: 15m 20s\tremaining: 46m 47s\n",
      "247:\tlearn: 14.8275389\ttotal: 15m 25s\tremaining: 46m 45s\n",
      "248:\tlearn: 14.8200205\ttotal: 15m 29s\tremaining: 46m 42s\n",
      "249:\tlearn: 14.8152574\ttotal: 15m 32s\tremaining: 46m 38s\n",
      "250:\tlearn: 14.8070948\ttotal: 15m 36s\tremaining: 46m 35s\n",
      "251:\tlearn: 14.8014521\ttotal: 15m 41s\tremaining: 46m 34s\n",
      "252:\tlearn: 14.7939843\ttotal: 15m 45s\tremaining: 46m 31s\n",
      "253:\tlearn: 14.7887285\ttotal: 15m 49s\tremaining: 46m 28s\n",
      "254:\tlearn: 14.7832613\ttotal: 15m 53s\tremaining: 46m 25s\n",
      "255:\tlearn: 14.7743223\ttotal: 15m 57s\tremaining: 46m 23s\n",
      "256:\tlearn: 14.7626896\ttotal: 16m 1s\tremaining: 46m 20s\n",
      "257:\tlearn: 14.7583353\ttotal: 16m 5s\tremaining: 46m 18s\n",
      "258:\tlearn: 14.7489148\ttotal: 16m 9s\tremaining: 46m 15s\n",
      "259:\tlearn: 14.7416740\ttotal: 16m 13s\tremaining: 46m 11s\n",
      "260:\tlearn: 14.7342102\ttotal: 16m 17s\tremaining: 46m 7s\n",
      "261:\tlearn: 14.7268448\ttotal: 16m 20s\tremaining: 46m 3s\n",
      "262:\tlearn: 14.7203416\ttotal: 16m 24s\tremaining: 45m 59s\n",
      "263:\tlearn: 14.7120191\ttotal: 16m 28s\tremaining: 45m 55s\n",
      "264:\tlearn: 14.7039152\ttotal: 16m 31s\tremaining: 45m 51s\n",
      "265:\tlearn: 14.6976013\ttotal: 16m 35s\tremaining: 45m 47s\n",
      "266:\tlearn: 14.6931374\ttotal: 16m 39s\tremaining: 45m 43s\n",
      "267:\tlearn: 14.6873042\ttotal: 16m 42s\tremaining: 45m 39s\n",
      "268:\tlearn: 14.6805603\ttotal: 16m 46s\tremaining: 45m 35s\n",
      "269:\tlearn: 14.6743299\ttotal: 16m 50s\tremaining: 45m 31s\n",
      "270:\tlearn: 14.6673805\ttotal: 16m 54s\tremaining: 45m 29s\n",
      "271:\tlearn: 14.6586746\ttotal: 16m 58s\tremaining: 45m 25s\n",
      "272:\tlearn: 14.6530354\ttotal: 17m 1s\tremaining: 45m 21s\n",
      "273:\tlearn: 14.6495727\ttotal: 17m 5s\tremaining: 45m 17s\n",
      "274:\tlearn: 14.6407141\ttotal: 17m 9s\tremaining: 45m 13s\n",
      "275:\tlearn: 14.6335922\ttotal: 17m 12s\tremaining: 45m 9s\n",
      "276:\tlearn: 14.6276010\ttotal: 17m 16s\tremaining: 45m 5s\n",
      "277:\tlearn: 14.6165453\ttotal: 17m 20s\tremaining: 45m 1s\n",
      "278:\tlearn: 14.6117454\ttotal: 17m 23s\tremaining: 44m 57s\n",
      "279:\tlearn: 14.6071956\ttotal: 17m 27s\tremaining: 44m 53s\n",
      "280:\tlearn: 14.5999575\ttotal: 17m 31s\tremaining: 44m 49s\n",
      "281:\tlearn: 14.5930265\ttotal: 17m 34s\tremaining: 44m 45s\n",
      "282:\tlearn: 14.5849862\ttotal: 17m 38s\tremaining: 44m 41s\n",
      "283:\tlearn: 14.5792502\ttotal: 17m 42s\tremaining: 44m 37s\n",
      "284:\tlearn: 14.5705175\ttotal: 17m 45s\tremaining: 44m 33s\n",
      "285:\tlearn: 14.5603445\ttotal: 17m 49s\tremaining: 44m 30s\n",
      "286:\tlearn: 14.5542561\ttotal: 17m 53s\tremaining: 44m 26s\n",
      "287:\tlearn: 14.5434050\ttotal: 17m 56s\tremaining: 44m 22s\n",
      "288:\tlearn: 14.5384933\ttotal: 18m\tremaining: 44m 18s\n",
      "289:\tlearn: 14.5308333\ttotal: 18m 4s\tremaining: 44m 14s\n",
      "290:\tlearn: 14.5266456\ttotal: 18m 7s\tremaining: 44m 10s\n",
      "291:\tlearn: 14.5215587\ttotal: 18m 11s\tremaining: 44m 6s\n",
      "292:\tlearn: 14.5154858\ttotal: 18m 15s\tremaining: 44m 2s\n",
      "293:\tlearn: 14.5074888\ttotal: 18m 18s\tremaining: 43m 58s\n",
      "294:\tlearn: 14.5026714\ttotal: 18m 22s\tremaining: 43m 54s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295:\tlearn: 14.4977063\ttotal: 18m 25s\tremaining: 43m 50s\n",
      "296:\tlearn: 14.4907194\ttotal: 18m 29s\tremaining: 43m 46s\n",
      "297:\tlearn: 14.4861064\ttotal: 18m 33s\tremaining: 43m 42s\n",
      "298:\tlearn: 14.4787839\ttotal: 18m 37s\tremaining: 43m 38s\n",
      "299:\tlearn: 14.4734660\ttotal: 18m 40s\tremaining: 43m 35s\n",
      "300:\tlearn: 14.4623839\ttotal: 18m 44s\tremaining: 43m 31s\n",
      "301:\tlearn: 14.4591753\ttotal: 18m 48s\tremaining: 43m 27s\n",
      "302:\tlearn: 14.4514251\ttotal: 18m 52s\tremaining: 43m 24s\n",
      "303:\tlearn: 14.4449683\ttotal: 18m 55s\tremaining: 43m 20s\n",
      "304:\tlearn: 14.4415502\ttotal: 18m 59s\tremaining: 43m 17s\n",
      "305:\tlearn: 14.4332097\ttotal: 19m 3s\tremaining: 43m 13s\n",
      "306:\tlearn: 14.4265674\ttotal: 19m 7s\tremaining: 43m 9s\n",
      "307:\tlearn: 14.4200405\ttotal: 19m 10s\tremaining: 43m 5s\n",
      "308:\tlearn: 14.4128040\ttotal: 19m 14s\tremaining: 43m 2s\n",
      "309:\tlearn: 14.4071874\ttotal: 19m 18s\tremaining: 42m 58s\n",
      "310:\tlearn: 14.4016994\ttotal: 19m 21s\tremaining: 42m 54s\n",
      "311:\tlearn: 14.3926597\ttotal: 19m 25s\tremaining: 42m 50s\n",
      "312:\tlearn: 14.3861968\ttotal: 19m 29s\tremaining: 42m 46s\n",
      "313:\tlearn: 14.3818340\ttotal: 19m 33s\tremaining: 42m 42s\n",
      "314:\tlearn: 14.3743799\ttotal: 19m 36s\tremaining: 42m 38s\n",
      "315:\tlearn: 14.3679446\ttotal: 19m 40s\tremaining: 42m 34s\n",
      "316:\tlearn: 14.3610884\ttotal: 19m 43s\tremaining: 42m 30s\n",
      "317:\tlearn: 14.3551062\ttotal: 19m 47s\tremaining: 42m 27s\n",
      "318:\tlearn: 14.3495938\ttotal: 19m 51s\tremaining: 42m 23s\n",
      "319:\tlearn: 14.3440867\ttotal: 19m 54s\tremaining: 42m 19s\n",
      "320:\tlearn: 14.3391899\ttotal: 19m 58s\tremaining: 42m 15s\n",
      "321:\tlearn: 14.3281407\ttotal: 20m 2s\tremaining: 42m 11s\n",
      "322:\tlearn: 14.3202831\ttotal: 20m 5s\tremaining: 42m 7s\n",
      "323:\tlearn: 14.3149749\ttotal: 20m 9s\tremaining: 42m 3s\n",
      "324:\tlearn: 14.3064317\ttotal: 20m 13s\tremaining: 41m 59s\n",
      "325:\tlearn: 14.2961123\ttotal: 20m 16s\tremaining: 41m 55s\n",
      "326:\tlearn: 14.2867749\ttotal: 20m 20s\tremaining: 41m 52s\n",
      "327:\tlearn: 14.2821615\ttotal: 20m 24s\tremaining: 41m 48s\n",
      "328:\tlearn: 14.2781327\ttotal: 20m 27s\tremaining: 41m 44s\n",
      "329:\tlearn: 14.2727269\ttotal: 20m 31s\tremaining: 41m 40s\n",
      "330:\tlearn: 14.2661249\ttotal: 20m 35s\tremaining: 41m 37s\n",
      "331:\tlearn: 14.2614460\ttotal: 20m 39s\tremaining: 41m 33s\n",
      "332:\tlearn: 14.2524325\ttotal: 20m 42s\tremaining: 41m 29s\n",
      "333:\tlearn: 14.2461834\ttotal: 20m 46s\tremaining: 41m 25s\n",
      "334:\tlearn: 14.2426069\ttotal: 20m 50s\tremaining: 41m 21s\n",
      "335:\tlearn: 14.2385785\ttotal: 20m 53s\tremaining: 41m 17s\n",
      "336:\tlearn: 14.2311992\ttotal: 20m 57s\tremaining: 41m 13s\n",
      "337:\tlearn: 14.2245754\ttotal: 21m 1s\tremaining: 41m 10s\n",
      "338:\tlearn: 14.2188397\ttotal: 21m 4s\tremaining: 41m 6s\n",
      "339:\tlearn: 14.2133877\ttotal: 21m 8s\tremaining: 41m 2s\n",
      "340:\tlearn: 14.2091864\ttotal: 21m 12s\tremaining: 40m 58s\n",
      "341:\tlearn: 14.2028312\ttotal: 21m 15s\tremaining: 40m 54s\n",
      "342:\tlearn: 14.1951146\ttotal: 21m 19s\tremaining: 40m 50s\n",
      "343:\tlearn: 14.1896564\ttotal: 21m 22s\tremaining: 40m 46s\n",
      "344:\tlearn: 14.1832323\ttotal: 21m 26s\tremaining: 40m 42s\n",
      "345:\tlearn: 14.1749898\ttotal: 21m 30s\tremaining: 40m 38s\n",
      "346:\tlearn: 14.1681268\ttotal: 21m 33s\tremaining: 40m 35s\n",
      "347:\tlearn: 14.1600918\ttotal: 21m 37s\tremaining: 40m 31s\n",
      "348:\tlearn: 14.1539385\ttotal: 21m 41s\tremaining: 40m 27s\n",
      "349:\tlearn: 14.1456448\ttotal: 21m 44s\tremaining: 40m 23s\n",
      "350:\tlearn: 14.1395722\ttotal: 21m 48s\tremaining: 40m 19s\n",
      "351:\tlearn: 14.1365965\ttotal: 21m 52s\tremaining: 40m 15s\n",
      "352:\tlearn: 14.1296504\ttotal: 21m 55s\tremaining: 40m 11s\n",
      "353:\tlearn: 14.1231325\ttotal: 21m 59s\tremaining: 40m 7s\n",
      "354:\tlearn: 14.1192710\ttotal: 22m 3s\tremaining: 40m 3s\n",
      "355:\tlearn: 14.1110906\ttotal: 22m 6s\tremaining: 40m\n",
      "356:\tlearn: 14.1032470\ttotal: 22m 10s\tremaining: 39m 56s\n",
      "357:\tlearn: 14.0961431\ttotal: 22m 14s\tremaining: 39m 52s\n",
      "358:\tlearn: 14.0912441\ttotal: 22m 17s\tremaining: 39m 48s\n",
      "359:\tlearn: 14.0872767\ttotal: 22m 21s\tremaining: 39m 44s\n",
      "360:\tlearn: 14.0828098\ttotal: 22m 24s\tremaining: 39m 40s\n",
      "361:\tlearn: 14.0779342\ttotal: 22m 28s\tremaining: 39m 36s\n",
      "362:\tlearn: 14.0716767\ttotal: 22m 32s\tremaining: 39m 32s\n",
      "363:\tlearn: 14.0676157\ttotal: 22m 35s\tremaining: 39m 29s\n",
      "364:\tlearn: 14.0636946\ttotal: 22m 39s\tremaining: 39m 24s\n",
      "365:\tlearn: 14.0576630\ttotal: 22m 43s\tremaining: 39m 21s\n",
      "366:\tlearn: 14.0512948\ttotal: 22m 46s\tremaining: 39m 17s\n",
      "367:\tlearn: 14.0456748\ttotal: 22m 50s\tremaining: 39m 13s\n",
      "368:\tlearn: 14.0368354\ttotal: 22m 54s\tremaining: 39m 9s\n",
      "369:\tlearn: 14.0323779\ttotal: 22m 57s\tremaining: 39m 5s\n",
      "370:\tlearn: 14.0273448\ttotal: 23m 1s\tremaining: 39m 1s\n",
      "371:\tlearn: 14.0192858\ttotal: 23m 4s\tremaining: 38m 58s\n",
      "372:\tlearn: 14.0153267\ttotal: 23m 8s\tremaining: 38m 54s\n",
      "373:\tlearn: 14.0106706\ttotal: 23m 12s\tremaining: 38m 50s\n",
      "374:\tlearn: 14.0070811\ttotal: 23m 16s\tremaining: 38m 46s\n",
      "375:\tlearn: 14.0025837\ttotal: 23m 19s\tremaining: 38m 42s\n",
      "376:\tlearn: 13.9980508\ttotal: 23m 23s\tremaining: 38m 39s\n",
      "377:\tlearn: 13.9915725\ttotal: 23m 27s\tremaining: 38m 35s\n",
      "378:\tlearn: 13.9852781\ttotal: 23m 30s\tremaining: 38m 31s\n",
      "379:\tlearn: 13.9789901\ttotal: 23m 34s\tremaining: 38m 27s\n",
      "380:\tlearn: 13.9741093\ttotal: 23m 38s\tremaining: 38m 23s\n",
      "381:\tlearn: 13.9681340\ttotal: 23m 41s\tremaining: 38m 20s\n",
      "382:\tlearn: 13.9608971\ttotal: 23m 45s\tremaining: 38m 16s\n",
      "383:\tlearn: 13.9527021\ttotal: 23m 49s\tremaining: 38m 12s\n",
      "384:\tlearn: 13.9459482\ttotal: 23m 52s\tremaining: 38m 8s\n",
      "385:\tlearn: 13.9418063\ttotal: 23m 56s\tremaining: 38m 4s\n",
      "386:\tlearn: 13.9354250\ttotal: 24m\tremaining: 38m\n",
      "387:\tlearn: 13.9292918\ttotal: 24m 3s\tremaining: 37m 57s\n",
      "388:\tlearn: 13.9234476\ttotal: 24m 7s\tremaining: 37m 53s\n",
      "389:\tlearn: 13.9195559\ttotal: 24m 11s\tremaining: 37m 49s\n",
      "390:\tlearn: 13.9121303\ttotal: 24m 14s\tremaining: 37m 45s\n",
      "391:\tlearn: 13.9021745\ttotal: 24m 18s\tremaining: 37m 42s\n",
      "392:\tlearn: 13.8969879\ttotal: 24m 22s\tremaining: 37m 38s\n",
      "393:\tlearn: 13.8915993\ttotal: 24m 25s\tremaining: 37m 34s\n",
      "394:\tlearn: 13.8850905\ttotal: 24m 29s\tremaining: 37m 30s\n",
      "395:\tlearn: 13.8757197\ttotal: 24m 33s\tremaining: 37m 26s\n",
      "396:\tlearn: 13.8686005\ttotal: 24m 36s\tremaining: 37m 23s\n",
      "397:\tlearn: 13.8642450\ttotal: 24m 40s\tremaining: 37m 19s\n",
      "398:\tlearn: 13.8540780\ttotal: 24m 44s\tremaining: 37m 15s\n",
      "399:\tlearn: 13.8480971\ttotal: 24m 47s\tremaining: 37m 11s\n",
      "400:\tlearn: 13.8441174\ttotal: 24m 51s\tremaining: 37m 7s\n",
      "401:\tlearn: 13.8387867\ttotal: 24m 55s\tremaining: 37m 4s\n",
      "402:\tlearn: 13.8343550\ttotal: 24m 58s\tremaining: 37m\n",
      "403:\tlearn: 13.8278825\ttotal: 25m 2s\tremaining: 36m 56s\n",
      "404:\tlearn: 13.8171124\ttotal: 25m 5s\tremaining: 36m 52s\n",
      "405:\tlearn: 13.8072492\ttotal: 25m 9s\tremaining: 36m 48s\n",
      "406:\tlearn: 13.8026092\ttotal: 25m 13s\tremaining: 36m 44s\n",
      "407:\tlearn: 13.7999004\ttotal: 25m 16s\tremaining: 36m 40s\n",
      "408:\tlearn: 13.7960624\ttotal: 25m 20s\tremaining: 36m 37s\n",
      "409:\tlearn: 13.7912907\ttotal: 25m 24s\tremaining: 36m 33s\n",
      "410:\tlearn: 13.7840919\ttotal: 25m 27s\tremaining: 36m 29s\n",
      "411:\tlearn: 13.7767711\ttotal: 25m 31s\tremaining: 36m 25s\n",
      "412:\tlearn: 13.7712787\ttotal: 25m 35s\tremaining: 36m 21s\n",
      "413:\tlearn: 13.7653068\ttotal: 25m 38s\tremaining: 36m 17s\n",
      "414:\tlearn: 13.7611754\ttotal: 25m 42s\tremaining: 36m 14s\n",
      "415:\tlearn: 13.7533864\ttotal: 25m 46s\tremaining: 36m 10s\n",
      "416:\tlearn: 13.7499105\ttotal: 25m 49s\tremaining: 36m 6s\n",
      "417:\tlearn: 13.7426998\ttotal: 25m 53s\tremaining: 36m 2s\n",
      "418:\tlearn: 13.7342017\ttotal: 25m 57s\tremaining: 35m 58s\n",
      "419:\tlearn: 13.7280912\ttotal: 26m\tremaining: 35m 55s\n",
      "420:\tlearn: 13.7192598\ttotal: 26m 4s\tremaining: 35m 51s\n",
      "421:\tlearn: 13.7146979\ttotal: 26m 8s\tremaining: 35m 47s\n",
      "422:\tlearn: 13.7061711\ttotal: 26m 11s\tremaining: 35m 43s\n",
      "423:\tlearn: 13.6978441\ttotal: 26m 15s\tremaining: 35m 40s\n",
      "424:\tlearn: 13.6924101\ttotal: 26m 19s\tremaining: 35m 36s\n",
      "425:\tlearn: 13.6841218\ttotal: 26m 22s\tremaining: 35m 32s\n",
      "426:\tlearn: 13.6784342\ttotal: 26m 26s\tremaining: 35m 28s\n",
      "427:\tlearn: 13.6724369\ttotal: 26m 30s\tremaining: 35m 24s\n",
      "428:\tlearn: 13.6683931\ttotal: 26m 33s\tremaining: 35m 21s\n",
      "429:\tlearn: 13.6646969\ttotal: 26m 37s\tremaining: 35m 17s\n",
      "430:\tlearn: 13.6611647\ttotal: 26m 40s\tremaining: 35m 13s\n",
      "431:\tlearn: 13.6583295\ttotal: 26m 44s\tremaining: 35m 9s\n",
      "432:\tlearn: 13.6538651\ttotal: 26m 48s\tremaining: 35m 5s\n",
      "433:\tlearn: 13.6478483\ttotal: 26m 51s\tremaining: 35m 1s\n",
      "434:\tlearn: 13.6379602\ttotal: 26m 55s\tremaining: 34m 58s\n",
      "435:\tlearn: 13.6294272\ttotal: 26m 59s\tremaining: 34m 54s\n",
      "436:\tlearn: 13.6242712\ttotal: 27m 2s\tremaining: 34m 50s\n",
      "437:\tlearn: 13.6191641\ttotal: 27m 6s\tremaining: 34m 46s\n",
      "438:\tlearn: 13.6135432\ttotal: 27m 10s\tremaining: 34m 43s\n",
      "439:\tlearn: 13.6096227\ttotal: 27m 13s\tremaining: 34m 39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440:\tlearn: 13.6058611\ttotal: 27m 17s\tremaining: 34m 35s\n",
      "441:\tlearn: 13.6024358\ttotal: 27m 20s\tremaining: 34m 31s\n",
      "442:\tlearn: 13.5971084\ttotal: 27m 24s\tremaining: 34m 27s\n",
      "443:\tlearn: 13.5897592\ttotal: 27m 28s\tremaining: 34m 23s\n",
      "444:\tlearn: 13.5820452\ttotal: 27m 31s\tremaining: 34m 20s\n",
      "445:\tlearn: 13.5739707\ttotal: 27m 35s\tremaining: 34m 16s\n",
      "446:\tlearn: 13.5683500\ttotal: 27m 39s\tremaining: 34m 13s\n",
      "447:\tlearn: 13.5644659\ttotal: 27m 43s\tremaining: 34m 9s\n",
      "448:\tlearn: 13.5587439\ttotal: 27m 46s\tremaining: 34m 5s\n",
      "449:\tlearn: 13.5551565\ttotal: 27m 50s\tremaining: 34m 1s\n",
      "450:\tlearn: 13.5472269\ttotal: 27m 54s\tremaining: 33m 58s\n",
      "451:\tlearn: 13.5412755\ttotal: 27m 57s\tremaining: 33m 54s\n",
      "452:\tlearn: 13.5358723\ttotal: 28m 1s\tremaining: 33m 50s\n",
      "453:\tlearn: 13.5285514\ttotal: 28m 5s\tremaining: 33m 46s\n",
      "454:\tlearn: 13.5253281\ttotal: 28m 8s\tremaining: 33m 42s\n",
      "455:\tlearn: 13.5194254\ttotal: 28m 12s\tremaining: 33m 39s\n",
      "456:\tlearn: 13.5131228\ttotal: 28m 16s\tremaining: 33m 35s\n",
      "457:\tlearn: 13.5064142\ttotal: 28m 19s\tremaining: 33m 31s\n",
      "458:\tlearn: 13.5011689\ttotal: 28m 23s\tremaining: 33m 27s\n",
      "459:\tlearn: 13.4959649\ttotal: 28m 27s\tremaining: 33m 23s\n",
      "460:\tlearn: 13.4912183\ttotal: 28m 30s\tremaining: 33m 20s\n",
      "461:\tlearn: 13.4870237\ttotal: 28m 34s\tremaining: 33m 16s\n",
      "462:\tlearn: 13.4826954\ttotal: 28m 38s\tremaining: 33m 12s\n",
      "463:\tlearn: 13.4784900\ttotal: 28m 41s\tremaining: 33m 8s\n",
      "464:\tlearn: 13.4727457\ttotal: 28m 45s\tremaining: 33m 5s\n",
      "465:\tlearn: 13.4664503\ttotal: 28m 49s\tremaining: 33m 1s\n",
      "466:\tlearn: 13.4616768\ttotal: 28m 52s\tremaining: 32m 57s\n",
      "467:\tlearn: 13.4558160\ttotal: 28m 56s\tremaining: 32m 54s\n",
      "468:\tlearn: 13.4506277\ttotal: 29m\tremaining: 32m 50s\n",
      "469:\tlearn: 13.4420394\ttotal: 29m 4s\tremaining: 32m 46s\n",
      "470:\tlearn: 13.4384504\ttotal: 29m 7s\tremaining: 32m 42s\n",
      "471:\tlearn: 13.4324000\ttotal: 29m 11s\tremaining: 32m 39s\n",
      "472:\tlearn: 13.4273385\ttotal: 29m 15s\tremaining: 32m 35s\n",
      "473:\tlearn: 13.4180254\ttotal: 29m 18s\tremaining: 32m 31s\n",
      "474:\tlearn: 13.4108743\ttotal: 29m 22s\tremaining: 32m 27s\n",
      "475:\tlearn: 13.4053213\ttotal: 29m 26s\tremaining: 32m 24s\n",
      "476:\tlearn: 13.3967272\ttotal: 29m 29s\tremaining: 32m 20s\n",
      "477:\tlearn: 13.3867956\ttotal: 29m 33s\tremaining: 32m 16s\n",
      "478:\tlearn: 13.3809398\ttotal: 29m 37s\tremaining: 32m 13s\n",
      "479:\tlearn: 13.3768067\ttotal: 29m 40s\tremaining: 32m 9s\n",
      "480:\tlearn: 13.3721651\ttotal: 29m 44s\tremaining: 32m 5s\n",
      "481:\tlearn: 13.3668164\ttotal: 29m 48s\tremaining: 32m 2s\n",
      "482:\tlearn: 13.3610548\ttotal: 29m 52s\tremaining: 31m 58s\n",
      "483:\tlearn: 13.3530130\ttotal: 29m 55s\tremaining: 31m 54s\n",
      "484:\tlearn: 13.3474734\ttotal: 29m 59s\tremaining: 31m 50s\n",
      "485:\tlearn: 13.3377312\ttotal: 30m 3s\tremaining: 31m 47s\n",
      "486:\tlearn: 13.3309545\ttotal: 30m 7s\tremaining: 31m 43s\n",
      "487:\tlearn: 13.3253511\ttotal: 30m 10s\tremaining: 31m 39s\n",
      "488:\tlearn: 13.3170969\ttotal: 30m 14s\tremaining: 31m 36s\n",
      "489:\tlearn: 13.3078425\ttotal: 30m 18s\tremaining: 31m 32s\n",
      "490:\tlearn: 13.3028652\ttotal: 30m 21s\tremaining: 31m 28s\n",
      "491:\tlearn: 13.2975386\ttotal: 30m 25s\tremaining: 31m 24s\n",
      "492:\tlearn: 13.2921805\ttotal: 30m 29s\tremaining: 31m 20s\n",
      "493:\tlearn: 13.2875517\ttotal: 30m 32s\tremaining: 31m 17s\n",
      "494:\tlearn: 13.2796075\ttotal: 30m 36s\tremaining: 31m 13s\n",
      "495:\tlearn: 13.2740663\ttotal: 30m 40s\tremaining: 31m 9s\n",
      "496:\tlearn: 13.2681220\ttotal: 30m 43s\tremaining: 31m 5s\n",
      "497:\tlearn: 13.2619728\ttotal: 30m 47s\tremaining: 31m 2s\n",
      "498:\tlearn: 13.2544469\ttotal: 30m 51s\tremaining: 30m 58s\n",
      "499:\tlearn: 13.2498003\ttotal: 30m 54s\tremaining: 30m 54s\n",
      "500:\tlearn: 13.2477752\ttotal: 30m 58s\tremaining: 30m 50s\n",
      "501:\tlearn: 13.2424159\ttotal: 31m 1s\tremaining: 30m 47s\n",
      "502:\tlearn: 13.2362328\ttotal: 31m 5s\tremaining: 30m 43s\n",
      "503:\tlearn: 13.2336018\ttotal: 31m 9s\tremaining: 30m 39s\n",
      "504:\tlearn: 13.2261929\ttotal: 31m 13s\tremaining: 30m 35s\n",
      "505:\tlearn: 13.2223415\ttotal: 31m 16s\tremaining: 30m 32s\n",
      "506:\tlearn: 13.2165590\ttotal: 31m 20s\tremaining: 30m 28s\n",
      "507:\tlearn: 13.2129560\ttotal: 31m 23s\tremaining: 30m 24s\n",
      "508:\tlearn: 13.2081921\ttotal: 31m 27s\tremaining: 30m 21s\n",
      "509:\tlearn: 13.2016480\ttotal: 31m 31s\tremaining: 30m 17s\n",
      "510:\tlearn: 13.1940664\ttotal: 31m 35s\tremaining: 30m 13s\n",
      "511:\tlearn: 13.1904025\ttotal: 31m 38s\tremaining: 30m 9s\n",
      "512:\tlearn: 13.1872361\ttotal: 31m 42s\tremaining: 30m 5s\n",
      "513:\tlearn: 13.1797810\ttotal: 31m 45s\tremaining: 30m 2s\n",
      "514:\tlearn: 13.1719791\ttotal: 31m 49s\tremaining: 29m 58s\n",
      "515:\tlearn: 13.1648098\ttotal: 31m 53s\tremaining: 29m 54s\n",
      "516:\tlearn: 13.1589328\ttotal: 31m 57s\tremaining: 29m 51s\n",
      "517:\tlearn: 13.1537936\ttotal: 32m\tremaining: 29m 47s\n",
      "518:\tlearn: 13.1518630\ttotal: 32m 4s\tremaining: 29m 43s\n",
      "519:\tlearn: 13.1484875\ttotal: 32m 7s\tremaining: 29m 39s\n",
      "520:\tlearn: 13.1415654\ttotal: 32m 11s\tremaining: 29m 35s\n",
      "521:\tlearn: 13.1319592\ttotal: 32m 15s\tremaining: 29m 32s\n",
      "522:\tlearn: 13.1231998\ttotal: 32m 19s\tremaining: 29m 28s\n",
      "523:\tlearn: 13.1165904\ttotal: 32m 22s\tremaining: 29m 24s\n",
      "524:\tlearn: 13.1124344\ttotal: 32m 26s\tremaining: 29m 20s\n",
      "525:\tlearn: 13.1078127\ttotal: 32m 29s\tremaining: 29m 17s\n",
      "526:\tlearn: 13.1004986\ttotal: 32m 33s\tremaining: 29m 13s\n",
      "527:\tlearn: 13.0946697\ttotal: 32m 37s\tremaining: 29m 9s\n",
      "528:\tlearn: 13.0911774\ttotal: 32m 40s\tremaining: 29m 5s\n",
      "529:\tlearn: 13.0880974\ttotal: 32m 44s\tremaining: 29m 2s\n",
      "530:\tlearn: 13.0832618\ttotal: 32m 48s\tremaining: 28m 58s\n",
      "531:\tlearn: 13.0768442\ttotal: 32m 51s\tremaining: 28m 54s\n",
      "532:\tlearn: 13.0717669\ttotal: 32m 55s\tremaining: 28m 50s\n",
      "533:\tlearn: 13.0613910\ttotal: 32m 59s\tremaining: 28m 47s\n",
      "534:\tlearn: 13.0569835\ttotal: 33m 2s\tremaining: 28m 43s\n",
      "535:\tlearn: 13.0523625\ttotal: 33m 6s\tremaining: 28m 39s\n",
      "536:\tlearn: 13.0498036\ttotal: 33m 10s\tremaining: 28m 35s\n",
      "537:\tlearn: 13.0411345\ttotal: 33m 13s\tremaining: 28m 32s\n",
      "538:\tlearn: 13.0325039\ttotal: 33m 17s\tremaining: 28m 28s\n",
      "539:\tlearn: 13.0300661\ttotal: 33m 21s\tremaining: 28m 24s\n",
      "540:\tlearn: 13.0256140\ttotal: 33m 24s\tremaining: 28m 20s\n",
      "541:\tlearn: 13.0193488\ttotal: 33m 28s\tremaining: 28m 17s\n",
      "542:\tlearn: 13.0134392\ttotal: 33m 32s\tremaining: 28m 13s\n",
      "543:\tlearn: 13.0036007\ttotal: 33m 35s\tremaining: 28m 9s\n",
      "544:\tlearn: 13.0007126\ttotal: 33m 39s\tremaining: 28m 5s\n",
      "545:\tlearn: 12.9947515\ttotal: 33m 43s\tremaining: 28m 2s\n",
      "546:\tlearn: 12.9908035\ttotal: 33m 46s\tremaining: 27m 58s\n",
      "547:\tlearn: 12.9883270\ttotal: 33m 50s\tremaining: 27m 54s\n",
      "548:\tlearn: 12.9814794\ttotal: 33m 54s\tremaining: 27m 50s\n",
      "549:\tlearn: 12.9717011\ttotal: 33m 57s\tremaining: 27m 47s\n",
      "550:\tlearn: 12.9646153\ttotal: 34m 1s\tremaining: 27m 43s\n",
      "551:\tlearn: 12.9568844\ttotal: 34m 5s\tremaining: 27m 39s\n",
      "552:\tlearn: 12.9530144\ttotal: 34m 8s\tremaining: 27m 36s\n",
      "553:\tlearn: 12.9502896\ttotal: 34m 12s\tremaining: 27m 32s\n",
      "554:\tlearn: 12.9437150\ttotal: 34m 16s\tremaining: 27m 28s\n",
      "555:\tlearn: 12.9361999\ttotal: 34m 19s\tremaining: 27m 24s\n",
      "556:\tlearn: 12.9314951\ttotal: 34m 23s\tremaining: 27m 21s\n",
      "557:\tlearn: 12.9272561\ttotal: 34m 27s\tremaining: 27m 17s\n",
      "558:\tlearn: 12.9228788\ttotal: 34m 30s\tremaining: 27m 13s\n",
      "559:\tlearn: 12.9203331\ttotal: 34m 34s\tremaining: 27m 9s\n",
      "560:\tlearn: 12.9154751\ttotal: 34m 37s\tremaining: 27m 6s\n",
      "561:\tlearn: 12.9122822\ttotal: 34m 41s\tremaining: 27m 2s\n",
      "562:\tlearn: 12.9076239\ttotal: 34m 45s\tremaining: 26m 58s\n",
      "563:\tlearn: 12.9034872\ttotal: 34m 48s\tremaining: 26m 54s\n",
      "564:\tlearn: 12.8950010\ttotal: 34m 52s\tremaining: 26m 51s\n",
      "565:\tlearn: 12.8907469\ttotal: 34m 56s\tremaining: 26m 47s\n",
      "566:\tlearn: 12.8823176\ttotal: 34m 59s\tremaining: 26m 43s\n",
      "567:\tlearn: 12.8790711\ttotal: 35m 3s\tremaining: 26m 39s\n",
      "568:\tlearn: 12.8729173\ttotal: 35m 7s\tremaining: 26m 36s\n",
      "569:\tlearn: 12.8674435\ttotal: 35m 10s\tremaining: 26m 32s\n",
      "570:\tlearn: 12.8604305\ttotal: 35m 14s\tremaining: 26m 28s\n",
      "571:\tlearn: 12.8581144\ttotal: 35m 18s\tremaining: 26m 25s\n",
      "572:\tlearn: 12.8514700\ttotal: 35m 22s\tremaining: 26m 21s\n",
      "573:\tlearn: 12.8496487\ttotal: 35m 25s\tremaining: 26m 17s\n",
      "574:\tlearn: 12.8443718\ttotal: 35m 29s\tremaining: 26m 13s\n",
      "575:\tlearn: 12.8389025\ttotal: 35m 32s\tremaining: 26m 10s\n",
      "576:\tlearn: 12.8350362\ttotal: 35m 36s\tremaining: 26m 6s\n",
      "577:\tlearn: 12.8328595\ttotal: 35m 40s\tremaining: 26m 2s\n",
      "578:\tlearn: 12.8277787\ttotal: 35m 43s\tremaining: 25m 58s\n",
      "579:\tlearn: 12.8197287\ttotal: 35m 47s\tremaining: 25m 55s\n",
      "580:\tlearn: 12.8133208\ttotal: 35m 51s\tremaining: 25m 51s\n",
      "581:\tlearn: 12.8091565\ttotal: 35m 54s\tremaining: 25m 47s\n",
      "582:\tlearn: 12.8032694\ttotal: 35m 58s\tremaining: 25m 43s\n",
      "583:\tlearn: 12.7966891\ttotal: 36m 1s\tremaining: 25m 40s\n",
      "584:\tlearn: 12.7901551\ttotal: 36m 5s\tremaining: 25m 36s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585:\tlearn: 12.7835195\ttotal: 36m 9s\tremaining: 25m 32s\n",
      "586:\tlearn: 12.7796032\ttotal: 36m 12s\tremaining: 25m 28s\n",
      "587:\tlearn: 12.7733943\ttotal: 36m 16s\tremaining: 25m 25s\n",
      "588:\tlearn: 12.7675911\ttotal: 36m 20s\tremaining: 25m 21s\n",
      "589:\tlearn: 12.7612911\ttotal: 36m 24s\tremaining: 25m 17s\n",
      "590:\tlearn: 12.7543684\ttotal: 36m 27s\tremaining: 25m 14s\n",
      "591:\tlearn: 12.7463769\ttotal: 36m 31s\tremaining: 25m 10s\n",
      "592:\tlearn: 12.7399981\ttotal: 36m 35s\tremaining: 25m 6s\n",
      "593:\tlearn: 12.7326868\ttotal: 36m 38s\tremaining: 25m 3s\n",
      "594:\tlearn: 12.7255853\ttotal: 36m 42s\tremaining: 24m 59s\n",
      "595:\tlearn: 12.7215187\ttotal: 36m 46s\tremaining: 24m 55s\n",
      "596:\tlearn: 12.7146877\ttotal: 36m 50s\tremaining: 24m 52s\n",
      "597:\tlearn: 12.7115343\ttotal: 36m 54s\tremaining: 24m 48s\n",
      "598:\tlearn: 12.7048604\ttotal: 36m 58s\tremaining: 24m 45s\n",
      "599:\tlearn: 12.6954432\ttotal: 37m 2s\tremaining: 24m 41s\n",
      "600:\tlearn: 12.6868579\ttotal: 37m 6s\tremaining: 24m 38s\n",
      "601:\tlearn: 12.6805752\ttotal: 37m 10s\tremaining: 24m 34s\n",
      "602:\tlearn: 12.6754141\ttotal: 37m 13s\tremaining: 24m 30s\n",
      "603:\tlearn: 12.6723790\ttotal: 37m 17s\tremaining: 24m 26s\n",
      "604:\tlearn: 12.6695217\ttotal: 37m 20s\tremaining: 24m 23s\n",
      "605:\tlearn: 12.6630016\ttotal: 37m 24s\tremaining: 24m 19s\n",
      "606:\tlearn: 12.6535154\ttotal: 37m 28s\tremaining: 24m 15s\n",
      "607:\tlearn: 12.6485771\ttotal: 37m 31s\tremaining: 24m 11s\n",
      "608:\tlearn: 12.6441093\ttotal: 37m 35s\tremaining: 24m 8s\n",
      "609:\tlearn: 12.6369351\ttotal: 37m 39s\tremaining: 24m 4s\n",
      "610:\tlearn: 12.6290775\ttotal: 37m 43s\tremaining: 24m 1s\n",
      "611:\tlearn: 12.6237608\ttotal: 37m 47s\tremaining: 23m 57s\n",
      "612:\tlearn: 12.6157633\ttotal: 37m 50s\tremaining: 23m 53s\n",
      "613:\tlearn: 12.6099980\ttotal: 37m 54s\tremaining: 23m 49s\n",
      "614:\tlearn: 12.6053425\ttotal: 37m 58s\tremaining: 23m 46s\n",
      "615:\tlearn: 12.5981204\ttotal: 38m 1s\tremaining: 23m 42s\n",
      "616:\tlearn: 12.5903847\ttotal: 38m 5s\tremaining: 23m 38s\n",
      "617:\tlearn: 12.5831524\ttotal: 38m 9s\tremaining: 23m 34s\n",
      "618:\tlearn: 12.5773897\ttotal: 38m 12s\tremaining: 23m 31s\n",
      "619:\tlearn: 12.5695500\ttotal: 38m 16s\tremaining: 23m 27s\n",
      "620:\tlearn: 12.5660640\ttotal: 38m 20s\tremaining: 23m 23s\n",
      "621:\tlearn: 12.5589507\ttotal: 38m 23s\tremaining: 23m 20s\n",
      "622:\tlearn: 12.5535526\ttotal: 38m 27s\tremaining: 23m 16s\n",
      "623:\tlearn: 12.5493816\ttotal: 38m 31s\tremaining: 23m 12s\n",
      "624:\tlearn: 12.5432767\ttotal: 38m 34s\tremaining: 23m 8s\n",
      "625:\tlearn: 12.5349527\ttotal: 38m 38s\tremaining: 23m 5s\n",
      "626:\tlearn: 12.5306488\ttotal: 38m 42s\tremaining: 23m 1s\n",
      "627:\tlearn: 12.5260551\ttotal: 38m 46s\tremaining: 22m 57s\n",
      "628:\tlearn: 12.5206769\ttotal: 38m 49s\tremaining: 22m 54s\n",
      "629:\tlearn: 12.5143948\ttotal: 38m 53s\tremaining: 22m 50s\n",
      "630:\tlearn: 12.5108494\ttotal: 38m 57s\tremaining: 22m 46s\n",
      "631:\tlearn: 12.5074270\ttotal: 39m 1s\tremaining: 22m 43s\n",
      "632:\tlearn: 12.4980884\ttotal: 39m 4s\tremaining: 22m 39s\n",
      "633:\tlearn: 12.4945007\ttotal: 39m 8s\tremaining: 22m 35s\n",
      "634:\tlearn: 12.4907482\ttotal: 39m 12s\tremaining: 22m 32s\n",
      "635:\tlearn: 12.4869570\ttotal: 39m 15s\tremaining: 22m 28s\n",
      "636:\tlearn: 12.4829759\ttotal: 39m 19s\tremaining: 22m 24s\n",
      "637:\tlearn: 12.4797868\ttotal: 39m 22s\tremaining: 22m 20s\n",
      "638:\tlearn: 12.4751037\ttotal: 39m 26s\tremaining: 22m 16s\n",
      "639:\tlearn: 12.4706870\ttotal: 39m 30s\tremaining: 22m 13s\n",
      "640:\tlearn: 12.4671028\ttotal: 39m 33s\tremaining: 22m 9s\n",
      "641:\tlearn: 12.4641035\ttotal: 39m 37s\tremaining: 22m 5s\n",
      "642:\tlearn: 12.4559761\ttotal: 39m 41s\tremaining: 22m 1s\n",
      "643:\tlearn: 12.4510320\ttotal: 39m 44s\tremaining: 21m 58s\n",
      "644:\tlearn: 12.4430019\ttotal: 39m 48s\tremaining: 21m 54s\n",
      "645:\tlearn: 12.4398012\ttotal: 39m 51s\tremaining: 21m 50s\n",
      "646:\tlearn: 12.4359578\ttotal: 39m 55s\tremaining: 21m 47s\n",
      "647:\tlearn: 12.4302817\ttotal: 39m 59s\tremaining: 21m 43s\n",
      "648:\tlearn: 12.4258607\ttotal: 40m 2s\tremaining: 21m 39s\n",
      "649:\tlearn: 12.4197843\ttotal: 40m 6s\tremaining: 21m 35s\n",
      "650:\tlearn: 12.4144216\ttotal: 40m 10s\tremaining: 21m 32s\n",
      "651:\tlearn: 12.4109584\ttotal: 40m 13s\tremaining: 21m 28s\n",
      "652:\tlearn: 12.4044484\ttotal: 40m 17s\tremaining: 21m 24s\n",
      "653:\tlearn: 12.3999666\ttotal: 40m 21s\tremaining: 21m 20s\n",
      "654:\tlearn: 12.3969014\ttotal: 40m 24s\tremaining: 21m 17s\n",
      "655:\tlearn: 12.3926674\ttotal: 40m 28s\tremaining: 21m 13s\n",
      "656:\tlearn: 12.3878092\ttotal: 40m 32s\tremaining: 21m 9s\n",
      "657:\tlearn: 12.3795122\ttotal: 40m 36s\tremaining: 21m 6s\n",
      "658:\tlearn: 12.3768248\ttotal: 40m 39s\tremaining: 21m 2s\n",
      "659:\tlearn: 12.3713019\ttotal: 40m 43s\tremaining: 20m 58s\n",
      "660:\tlearn: 12.3648270\ttotal: 40m 47s\tremaining: 20m 55s\n",
      "661:\tlearn: 12.3603244\ttotal: 40m 50s\tremaining: 20m 51s\n",
      "662:\tlearn: 12.3566612\ttotal: 40m 54s\tremaining: 20m 47s\n",
      "663:\tlearn: 12.3519026\ttotal: 40m 57s\tremaining: 20m 43s\n",
      "664:\tlearn: 12.3493872\ttotal: 41m 1s\tremaining: 20m 40s\n",
      "665:\tlearn: 12.3463830\ttotal: 41m 5s\tremaining: 20m 36s\n",
      "666:\tlearn: 12.3421446\ttotal: 41m 8s\tremaining: 20m 32s\n",
      "667:\tlearn: 12.3358172\ttotal: 41m 12s\tremaining: 20m 28s\n",
      "668:\tlearn: 12.3301693\ttotal: 41m 16s\tremaining: 20m 25s\n",
      "669:\tlearn: 12.3246423\ttotal: 41m 19s\tremaining: 20m 21s\n",
      "670:\tlearn: 12.3204374\ttotal: 41m 23s\tremaining: 20m 17s\n",
      "671:\tlearn: 12.3164382\ttotal: 41m 26s\tremaining: 20m 13s\n",
      "672:\tlearn: 12.3095740\ttotal: 41m 30s\tremaining: 20m 10s\n",
      "673:\tlearn: 12.3054093\ttotal: 41m 34s\tremaining: 20m 6s\n",
      "674:\tlearn: 12.2970384\ttotal: 41m 37s\tremaining: 20m 2s\n",
      "675:\tlearn: 12.2918657\ttotal: 41m 41s\tremaining: 19m 59s\n",
      "676:\tlearn: 12.2874581\ttotal: 41m 45s\tremaining: 19m 55s\n",
      "677:\tlearn: 12.2793942\ttotal: 41m 49s\tremaining: 19m 51s\n",
      "678:\tlearn: 12.2710597\ttotal: 41m 52s\tremaining: 19m 47s\n",
      "679:\tlearn: 12.2671278\ttotal: 41m 56s\tremaining: 19m 44s\n",
      "680:\tlearn: 12.2636086\ttotal: 41m 59s\tremaining: 19m 40s\n",
      "681:\tlearn: 12.2603348\ttotal: 42m 3s\tremaining: 19m 36s\n",
      "682:\tlearn: 12.2569573\ttotal: 42m 7s\tremaining: 19m 32s\n",
      "683:\tlearn: 12.2497504\ttotal: 42m 10s\tremaining: 19m 29s\n",
      "684:\tlearn: 12.2465685\ttotal: 42m 14s\tremaining: 19m 25s\n",
      "685:\tlearn: 12.2429887\ttotal: 42m 18s\tremaining: 19m 21s\n",
      "686:\tlearn: 12.2390000\ttotal: 42m 21s\tremaining: 19m 17s\n",
      "687:\tlearn: 12.2325100\ttotal: 42m 25s\tremaining: 19m 14s\n",
      "688:\tlearn: 12.2251170\ttotal: 42m 28s\tremaining: 19m 10s\n",
      "689:\tlearn: 12.2220792\ttotal: 42m 32s\tremaining: 19m 6s\n",
      "690:\tlearn: 12.2173300\ttotal: 42m 36s\tremaining: 19m 3s\n",
      "691:\tlearn: 12.2112209\ttotal: 42m 39s\tremaining: 18m 59s\n",
      "692:\tlearn: 12.2067672\ttotal: 42m 43s\tremaining: 18m 55s\n",
      "693:\tlearn: 12.2035272\ttotal: 42m 47s\tremaining: 18m 52s\n",
      "694:\tlearn: 12.1989347\ttotal: 42m 51s\tremaining: 18m 48s\n",
      "695:\tlearn: 12.1944504\ttotal: 42m 55s\tremaining: 18m 44s\n",
      "696:\tlearn: 12.1909430\ttotal: 42m 59s\tremaining: 18m 41s\n",
      "697:\tlearn: 12.1878009\ttotal: 43m 3s\tremaining: 18m 37s\n",
      "698:\tlearn: 12.1843009\ttotal: 43m 6s\tremaining: 18m 33s\n",
      "699:\tlearn: 12.1792958\ttotal: 43m 10s\tremaining: 18m 30s\n",
      "700:\tlearn: 12.1754270\ttotal: 43m 14s\tremaining: 18m 26s\n",
      "701:\tlearn: 12.1724298\ttotal: 43m 18s\tremaining: 18m 22s\n",
      "702:\tlearn: 12.1691537\ttotal: 43m 21s\tremaining: 18m 19s\n",
      "703:\tlearn: 12.1630137\ttotal: 43m 25s\tremaining: 18m 15s\n",
      "704:\tlearn: 12.1567117\ttotal: 43m 29s\tremaining: 18m 11s\n",
      "705:\tlearn: 12.1486885\ttotal: 43m 32s\tremaining: 18m 8s\n",
      "706:\tlearn: 12.1419978\ttotal: 43m 36s\tremaining: 18m 4s\n",
      "707:\tlearn: 12.1340813\ttotal: 43m 40s\tremaining: 18m\n",
      "708:\tlearn: 12.1305563\ttotal: 43m 44s\tremaining: 17m 57s\n",
      "709:\tlearn: 12.1245378\ttotal: 43m 47s\tremaining: 17m 53s\n",
      "710:\tlearn: 12.1212456\ttotal: 43m 51s\tremaining: 17m 49s\n",
      "711:\tlearn: 12.1143165\ttotal: 43m 55s\tremaining: 17m 45s\n",
      "712:\tlearn: 12.1104604\ttotal: 43m 58s\tremaining: 17m 42s\n",
      "713:\tlearn: 12.1069966\ttotal: 44m 2s\tremaining: 17m 38s\n",
      "714:\tlearn: 12.1004730\ttotal: 44m 5s\tremaining: 17m 34s\n",
      "715:\tlearn: 12.0970231\ttotal: 44m 9s\tremaining: 17m 30s\n",
      "716:\tlearn: 12.0917140\ttotal: 44m 13s\tremaining: 17m 27s\n",
      "717:\tlearn: 12.0879015\ttotal: 44m 16s\tremaining: 17m 23s\n",
      "718:\tlearn: 12.0841359\ttotal: 44m 20s\tremaining: 17m 19s\n",
      "719:\tlearn: 12.0801948\ttotal: 44m 24s\tremaining: 17m 16s\n",
      "720:\tlearn: 12.0761171\ttotal: 44m 27s\tremaining: 17m 12s\n",
      "721:\tlearn: 12.0693647\ttotal: 44m 31s\tremaining: 17m 8s\n",
      "722:\tlearn: 12.0644941\ttotal: 44m 35s\tremaining: 17m 4s\n",
      "723:\tlearn: 12.0599462\ttotal: 44m 38s\tremaining: 17m 1s\n",
      "724:\tlearn: 12.0561787\ttotal: 44m 42s\tremaining: 16m 57s\n",
      "725:\tlearn: 12.0493525\ttotal: 44m 46s\tremaining: 16m 53s\n",
      "726:\tlearn: 12.0459041\ttotal: 44m 49s\tremaining: 16m 50s\n",
      "727:\tlearn: 12.0429070\ttotal: 44m 53s\tremaining: 16m 46s\n",
      "728:\tlearn: 12.0393309\ttotal: 44m 57s\tremaining: 16m 42s\n",
      "729:\tlearn: 12.0354859\ttotal: 45m 1s\tremaining: 16m 39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730:\tlearn: 12.0304143\ttotal: 45m 5s\tremaining: 16m 35s\n",
      "731:\tlearn: 12.0239337\ttotal: 45m 9s\tremaining: 16m 32s\n",
      "732:\tlearn: 12.0167930\ttotal: 45m 13s\tremaining: 16m 28s\n",
      "733:\tlearn: 12.0119768\ttotal: 45m 16s\tremaining: 16m 24s\n",
      "734:\tlearn: 12.0050569\ttotal: 45m 20s\tremaining: 16m 20s\n",
      "735:\tlearn: 12.0025165\ttotal: 45m 24s\tremaining: 16m 17s\n",
      "736:\tlearn: 11.9936276\ttotal: 45m 27s\tremaining: 16m 13s\n",
      "737:\tlearn: 11.9880381\ttotal: 45m 31s\tremaining: 16m 9s\n",
      "738:\tlearn: 11.9842453\ttotal: 45m 35s\tremaining: 16m 6s\n",
      "739:\tlearn: 11.9791067\ttotal: 45m 38s\tremaining: 16m 2s\n",
      "740:\tlearn: 11.9744595\ttotal: 45m 42s\tremaining: 15m 58s\n",
      "741:\tlearn: 11.9675259\ttotal: 45m 46s\tremaining: 15m 54s\n",
      "742:\tlearn: 11.9617911\ttotal: 45m 49s\tremaining: 15m 51s\n",
      "743:\tlearn: 11.9558033\ttotal: 45m 53s\tremaining: 15m 47s\n",
      "744:\tlearn: 11.9521999\ttotal: 45m 57s\tremaining: 15m 43s\n",
      "745:\tlearn: 11.9468197\ttotal: 46m\tremaining: 15m 39s\n",
      "746:\tlearn: 11.9430473\ttotal: 46m 4s\tremaining: 15m 36s\n",
      "747:\tlearn: 11.9395957\ttotal: 46m 7s\tremaining: 15m 32s\n",
      "748:\tlearn: 11.9360021\ttotal: 46m 11s\tremaining: 15m 28s\n",
      "749:\tlearn: 11.9310589\ttotal: 46m 15s\tremaining: 15m 25s\n",
      "750:\tlearn: 11.9279432\ttotal: 46m 18s\tremaining: 15m 21s\n",
      "751:\tlearn: 11.9228701\ttotal: 46m 22s\tremaining: 15m 17s\n",
      "752:\tlearn: 11.9183186\ttotal: 46m 26s\tremaining: 15m 13s\n",
      "753:\tlearn: 11.9156196\ttotal: 46m 29s\tremaining: 15m 10s\n",
      "754:\tlearn: 11.9124656\ttotal: 46m 33s\tremaining: 15m 6s\n",
      "755:\tlearn: 11.9058848\ttotal: 46m 37s\tremaining: 15m 2s\n",
      "756:\tlearn: 11.8994866\ttotal: 46m 40s\tremaining: 14m 59s\n",
      "757:\tlearn: 11.8958506\ttotal: 46m 44s\tremaining: 14m 55s\n",
      "758:\tlearn: 11.8890966\ttotal: 46m 48s\tremaining: 14m 51s\n",
      "759:\tlearn: 11.8852273\ttotal: 46m 51s\tremaining: 14m 47s\n",
      "760:\tlearn: 11.8804575\ttotal: 46m 55s\tremaining: 14m 44s\n",
      "761:\tlearn: 11.8764147\ttotal: 46m 59s\tremaining: 14m 40s\n",
      "762:\tlearn: 11.8730430\ttotal: 47m 2s\tremaining: 14m 36s\n",
      "763:\tlearn: 11.8694441\ttotal: 47m 6s\tremaining: 14m 33s\n",
      "764:\tlearn: 11.8659604\ttotal: 47m 10s\tremaining: 14m 29s\n",
      "765:\tlearn: 11.8605699\ttotal: 47m 13s\tremaining: 14m 25s\n",
      "766:\tlearn: 11.8520520\ttotal: 47m 17s\tremaining: 14m 22s\n",
      "767:\tlearn: 11.8460258\ttotal: 47m 21s\tremaining: 14m 18s\n",
      "768:\tlearn: 11.8409544\ttotal: 47m 25s\tremaining: 14m 14s\n",
      "769:\tlearn: 11.8356255\ttotal: 47m 28s\tremaining: 14m 10s\n",
      "770:\tlearn: 11.8308505\ttotal: 47m 32s\tremaining: 14m 7s\n",
      "771:\tlearn: 11.8269028\ttotal: 47m 35s\tremaining: 14m 3s\n",
      "772:\tlearn: 11.8232903\ttotal: 47m 39s\tremaining: 13m 59s\n",
      "773:\tlearn: 11.8197749\ttotal: 47m 43s\tremaining: 13m 56s\n",
      "774:\tlearn: 11.8155027\ttotal: 47m 46s\tremaining: 13m 52s\n",
      "775:\tlearn: 11.8119032\ttotal: 47m 50s\tremaining: 13m 48s\n",
      "776:\tlearn: 11.8089247\ttotal: 47m 53s\tremaining: 13m 44s\n",
      "777:\tlearn: 11.8013882\ttotal: 47m 57s\tremaining: 13m 41s\n",
      "778:\tlearn: 11.7967523\ttotal: 48m 1s\tremaining: 13m 37s\n",
      "779:\tlearn: 11.7904027\ttotal: 48m 4s\tremaining: 13m 33s\n",
      "780:\tlearn: 11.7840829\ttotal: 48m 8s\tremaining: 13m 29s\n",
      "781:\tlearn: 11.7793775\ttotal: 48m 12s\tremaining: 13m 26s\n",
      "782:\tlearn: 11.7764932\ttotal: 48m 15s\tremaining: 13m 22s\n",
      "783:\tlearn: 11.7736919\ttotal: 48m 19s\tremaining: 13m 18s\n",
      "784:\tlearn: 11.7715130\ttotal: 48m 23s\tremaining: 13m 15s\n",
      "785:\tlearn: 11.7669384\ttotal: 48m 26s\tremaining: 13m 11s\n",
      "786:\tlearn: 11.7642708\ttotal: 48m 30s\tremaining: 13m 7s\n",
      "787:\tlearn: 11.7565600\ttotal: 48m 34s\tremaining: 13m 3s\n",
      "788:\tlearn: 11.7548096\ttotal: 48m 37s\tremaining: 13m\n",
      "789:\tlearn: 11.7505440\ttotal: 48m 41s\tremaining: 12m 56s\n",
      "790:\tlearn: 11.7432393\ttotal: 48m 45s\tremaining: 12m 52s\n",
      "791:\tlearn: 11.7405806\ttotal: 48m 48s\tremaining: 12m 49s\n",
      "792:\tlearn: 11.7352617\ttotal: 48m 52s\tremaining: 12m 45s\n",
      "793:\tlearn: 11.7300811\ttotal: 48m 56s\tremaining: 12m 41s\n",
      "794:\tlearn: 11.7214814\ttotal: 49m\tremaining: 12m 38s\n",
      "795:\tlearn: 11.7145664\ttotal: 49m 3s\tremaining: 12m 34s\n",
      "796:\tlearn: 11.7106548\ttotal: 49m 7s\tremaining: 12m 30s\n",
      "797:\tlearn: 11.7075743\ttotal: 49m 11s\tremaining: 12m 27s\n",
      "798:\tlearn: 11.7015536\ttotal: 49m 14s\tremaining: 12m 23s\n",
      "799:\tlearn: 11.6943751\ttotal: 49m 18s\tremaining: 12m 19s\n",
      "800:\tlearn: 11.6866737\ttotal: 49m 22s\tremaining: 12m 15s\n",
      "801:\tlearn: 11.6834106\ttotal: 49m 26s\tremaining: 12m 12s\n",
      "802:\tlearn: 11.6757802\ttotal: 49m 29s\tremaining: 12m 8s\n",
      "803:\tlearn: 11.6704868\ttotal: 49m 33s\tremaining: 12m 4s\n",
      "804:\tlearn: 11.6657226\ttotal: 49m 37s\tremaining: 12m 1s\n",
      "805:\tlearn: 11.6607512\ttotal: 49m 41s\tremaining: 11m 57s\n",
      "806:\tlearn: 11.6573155\ttotal: 49m 44s\tremaining: 11m 53s\n",
      "807:\tlearn: 11.6500353\ttotal: 49m 48s\tremaining: 11m 50s\n",
      "808:\tlearn: 11.6447010\ttotal: 49m 52s\tremaining: 11m 46s\n",
      "809:\tlearn: 11.6416644\ttotal: 49m 55s\tremaining: 11m 42s\n",
      "810:\tlearn: 11.6345369\ttotal: 49m 59s\tremaining: 11m 38s\n",
      "811:\tlearn: 11.6316098\ttotal: 50m 2s\tremaining: 11m 35s\n",
      "812:\tlearn: 11.6247990\ttotal: 50m 6s\tremaining: 11m 31s\n",
      "813:\tlearn: 11.6195056\ttotal: 50m 10s\tremaining: 11m 27s\n",
      "814:\tlearn: 11.6153069\ttotal: 50m 13s\tremaining: 11m 24s\n",
      "815:\tlearn: 11.6123042\ttotal: 50m 17s\tremaining: 11m 20s\n",
      "816:\tlearn: 11.6075195\ttotal: 50m 21s\tremaining: 11m 16s\n",
      "817:\tlearn: 11.6028058\ttotal: 50m 24s\tremaining: 11m 12s\n",
      "818:\tlearn: 11.5964380\ttotal: 50m 28s\tremaining: 11m 9s\n",
      "819:\tlearn: 11.5895306\ttotal: 50m 32s\tremaining: 11m 5s\n",
      "820:\tlearn: 11.5856998\ttotal: 50m 35s\tremaining: 11m 1s\n",
      "821:\tlearn: 11.5810533\ttotal: 50m 39s\tremaining: 10m 58s\n",
      "822:\tlearn: 11.5743308\ttotal: 50m 43s\tremaining: 10m 54s\n",
      "823:\tlearn: 11.5671518\ttotal: 50m 46s\tremaining: 10m 50s\n",
      "824:\tlearn: 11.5633958\ttotal: 50m 50s\tremaining: 10m 47s\n",
      "825:\tlearn: 11.5557106\ttotal: 50m 54s\tremaining: 10m 43s\n",
      "826:\tlearn: 11.5488282\ttotal: 50m 57s\tremaining: 10m 39s\n",
      "827:\tlearn: 11.5443795\ttotal: 51m 1s\tremaining: 10m 35s\n",
      "828:\tlearn: 11.5379408\ttotal: 51m 5s\tremaining: 10m 32s\n",
      "829:\tlearn: 11.5303256\ttotal: 51m 8s\tremaining: 10m 28s\n",
      "830:\tlearn: 11.5246393\ttotal: 51m 12s\tremaining: 10m 24s\n",
      "831:\tlearn: 11.5184265\ttotal: 51m 16s\tremaining: 10m 21s\n",
      "832:\tlearn: 11.5141068\ttotal: 51m 19s\tremaining: 10m 17s\n",
      "833:\tlearn: 11.5103745\ttotal: 51m 23s\tremaining: 10m 13s\n",
      "834:\tlearn: 11.5077742\ttotal: 51m 27s\tremaining: 10m 10s\n",
      "835:\tlearn: 11.5000302\ttotal: 51m 30s\tremaining: 10m 6s\n",
      "836:\tlearn: 11.4943665\ttotal: 51m 34s\tremaining: 10m 2s\n",
      "837:\tlearn: 11.4907482\ttotal: 51m 38s\tremaining: 9m 58s\n",
      "838:\tlearn: 11.4889291\ttotal: 51m 41s\tremaining: 9m 55s\n",
      "839:\tlearn: 11.4862839\ttotal: 51m 45s\tremaining: 9m 51s\n",
      "840:\tlearn: 11.4776176\ttotal: 51m 48s\tremaining: 9m 47s\n",
      "841:\tlearn: 11.4739184\ttotal: 51m 52s\tremaining: 9m 44s\n",
      "842:\tlearn: 11.4667289\ttotal: 51m 56s\tremaining: 9m 40s\n",
      "843:\tlearn: 11.4634587\ttotal: 51m 59s\tremaining: 9m 36s\n",
      "844:\tlearn: 11.4585847\ttotal: 52m 3s\tremaining: 9m 32s\n",
      "845:\tlearn: 11.4527766\ttotal: 52m 7s\tremaining: 9m 29s\n",
      "846:\tlearn: 11.4469478\ttotal: 52m 10s\tremaining: 9m 25s\n",
      "847:\tlearn: 11.4412961\ttotal: 52m 14s\tremaining: 9m 21s\n",
      "848:\tlearn: 11.4381518\ttotal: 52m 18s\tremaining: 9m 18s\n",
      "849:\tlearn: 11.4294355\ttotal: 52m 22s\tremaining: 9m 14s\n",
      "850:\tlearn: 11.4271806\ttotal: 52m 25s\tremaining: 9m 10s\n",
      "851:\tlearn: 11.4203941\ttotal: 52m 29s\tremaining: 9m 7s\n",
      "852:\tlearn: 11.4177164\ttotal: 52m 33s\tremaining: 9m 3s\n",
      "853:\tlearn: 11.4137820\ttotal: 52m 36s\tremaining: 8m 59s\n",
      "854:\tlearn: 11.4086135\ttotal: 52m 40s\tremaining: 8m 55s\n",
      "855:\tlearn: 11.4057776\ttotal: 52m 44s\tremaining: 8m 52s\n",
      "856:\tlearn: 11.4030874\ttotal: 52m 47s\tremaining: 8m 48s\n",
      "857:\tlearn: 11.3986892\ttotal: 52m 51s\tremaining: 8m 44s\n",
      "858:\tlearn: 11.3962818\ttotal: 52m 54s\tremaining: 8m 41s\n",
      "859:\tlearn: 11.3928128\ttotal: 52m 58s\tremaining: 8m 37s\n",
      "860:\tlearn: 11.3894181\ttotal: 53m 2s\tremaining: 8m 33s\n",
      "861:\tlearn: 11.3817735\ttotal: 53m 6s\tremaining: 8m 30s\n",
      "862:\tlearn: 11.3788702\ttotal: 53m 9s\tremaining: 8m 26s\n",
      "863:\tlearn: 11.3750696\ttotal: 53m 13s\tremaining: 8m 22s\n",
      "864:\tlearn: 11.3696621\ttotal: 53m 17s\tremaining: 8m 18s\n",
      "865:\tlearn: 11.3666214\ttotal: 53m 20s\tremaining: 8m 15s\n",
      "866:\tlearn: 11.3614352\ttotal: 53m 24s\tremaining: 8m 11s\n",
      "867:\tlearn: 11.3585191\ttotal: 53m 27s\tremaining: 8m 7s\n",
      "868:\tlearn: 11.3545157\ttotal: 53m 31s\tremaining: 8m 4s\n",
      "869:\tlearn: 11.3522105\ttotal: 53m 35s\tremaining: 8m\n",
      "870:\tlearn: 11.3434942\ttotal: 53m 38s\tremaining: 7m 56s\n",
      "871:\tlearn: 11.3401544\ttotal: 53m 42s\tremaining: 7m 53s\n",
      "872:\tlearn: 11.3333507\ttotal: 53m 46s\tremaining: 7m 49s\n",
      "873:\tlearn: 11.3262342\ttotal: 53m 50s\tremaining: 7m 45s\n",
      "874:\tlearn: 11.3238491\ttotal: 53m 53s\tremaining: 7m 41s\n",
      "875:\tlearn: 11.3206699\ttotal: 53m 57s\tremaining: 7m 38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876:\tlearn: 11.3152596\ttotal: 54m\tremaining: 7m 34s\n",
      "877:\tlearn: 11.3128553\ttotal: 54m 4s\tremaining: 7m 30s\n",
      "878:\tlearn: 11.3066828\ttotal: 54m 8s\tremaining: 7m 27s\n",
      "879:\tlearn: 11.3041465\ttotal: 54m 11s\tremaining: 7m 23s\n",
      "880:\tlearn: 11.2998677\ttotal: 54m 15s\tremaining: 7m 19s\n",
      "881:\tlearn: 11.2932036\ttotal: 54m 19s\tremaining: 7m 16s\n",
      "882:\tlearn: 11.2910163\ttotal: 54m 22s\tremaining: 7m 12s\n",
      "883:\tlearn: 11.2879324\ttotal: 54m 26s\tremaining: 7m 8s\n",
      "884:\tlearn: 11.2842683\ttotal: 54m 29s\tremaining: 7m 4s\n",
      "885:\tlearn: 11.2792487\ttotal: 54m 33s\tremaining: 7m 1s\n",
      "886:\tlearn: 11.2725564\ttotal: 54m 37s\tremaining: 6m 57s\n",
      "887:\tlearn: 11.2667709\ttotal: 54m 41s\tremaining: 6m 53s\n",
      "888:\tlearn: 11.2626219\ttotal: 54m 44s\tremaining: 6m 50s\n",
      "889:\tlearn: 11.2565498\ttotal: 54m 48s\tremaining: 6m 46s\n",
      "890:\tlearn: 11.2508487\ttotal: 54m 52s\tremaining: 6m 42s\n",
      "891:\tlearn: 11.2431884\ttotal: 54m 55s\tremaining: 6m 39s\n",
      "892:\tlearn: 11.2387651\ttotal: 54m 59s\tremaining: 6m 35s\n",
      "893:\tlearn: 11.2343789\ttotal: 55m 3s\tremaining: 6m 31s\n",
      "894:\tlearn: 11.2300375\ttotal: 55m 6s\tremaining: 6m 27s\n",
      "895:\tlearn: 11.2257653\ttotal: 55m 10s\tremaining: 6m 24s\n",
      "896:\tlearn: 11.2183401\ttotal: 55m 14s\tremaining: 6m 20s\n",
      "897:\tlearn: 11.2136884\ttotal: 55m 17s\tremaining: 6m 16s\n",
      "898:\tlearn: 11.2107040\ttotal: 55m 21s\tremaining: 6m 13s\n",
      "899:\tlearn: 11.2080521\ttotal: 55m 24s\tremaining: 6m 9s\n",
      "900:\tlearn: 11.2038756\ttotal: 55m 28s\tremaining: 6m 5s\n",
      "901:\tlearn: 11.1989955\ttotal: 55m 32s\tremaining: 6m 2s\n",
      "902:\tlearn: 11.1925296\ttotal: 55m 36s\tremaining: 5m 58s\n",
      "903:\tlearn: 11.1869066\ttotal: 55m 39s\tremaining: 5m 54s\n",
      "904:\tlearn: 11.1826363\ttotal: 55m 43s\tremaining: 5m 50s\n",
      "905:\tlearn: 11.1763734\ttotal: 55m 47s\tremaining: 5m 47s\n",
      "906:\tlearn: 11.1730389\ttotal: 55m 50s\tremaining: 5m 43s\n",
      "907:\tlearn: 11.1695535\ttotal: 55m 54s\tremaining: 5m 39s\n",
      "908:\tlearn: 11.1661626\ttotal: 55m 57s\tremaining: 5m 36s\n",
      "909:\tlearn: 11.1612609\ttotal: 56m 1s\tremaining: 5m 32s\n",
      "910:\tlearn: 11.1547363\ttotal: 56m 5s\tremaining: 5m 28s\n",
      "911:\tlearn: 11.1504059\ttotal: 56m 8s\tremaining: 5m 25s\n",
      "912:\tlearn: 11.1440960\ttotal: 56m 12s\tremaining: 5m 21s\n",
      "913:\tlearn: 11.1417576\ttotal: 56m 16s\tremaining: 5m 17s\n",
      "914:\tlearn: 11.1372947\ttotal: 56m 19s\tremaining: 5m 13s\n",
      "915:\tlearn: 11.1335332\ttotal: 56m 23s\tremaining: 5m 10s\n",
      "916:\tlearn: 11.1289767\ttotal: 56m 26s\tremaining: 5m 6s\n",
      "917:\tlearn: 11.1253699\ttotal: 56m 30s\tremaining: 5m 2s\n",
      "918:\tlearn: 11.1214633\ttotal: 56m 34s\tremaining: 4m 59s\n",
      "919:\tlearn: 11.1173983\ttotal: 56m 37s\tremaining: 4m 55s\n",
      "920:\tlearn: 11.1123349\ttotal: 56m 41s\tremaining: 4m 51s\n",
      "921:\tlearn: 11.1086397\ttotal: 56m 44s\tremaining: 4m 48s\n",
      "922:\tlearn: 11.1057989\ttotal: 56m 48s\tremaining: 4m 44s\n",
      "923:\tlearn: 11.1025721\ttotal: 56m 52s\tremaining: 4m 40s\n",
      "924:\tlearn: 11.0990399\ttotal: 56m 55s\tremaining: 4m 36s\n",
      "925:\tlearn: 11.0946587\ttotal: 56m 59s\tremaining: 4m 33s\n",
      "926:\tlearn: 11.0895742\ttotal: 57m 3s\tremaining: 4m 29s\n",
      "927:\tlearn: 11.0862374\ttotal: 57m 6s\tremaining: 4m 25s\n",
      "928:\tlearn: 11.0819794\ttotal: 57m 10s\tremaining: 4m 22s\n",
      "929:\tlearn: 11.0797408\ttotal: 57m 14s\tremaining: 4m 18s\n",
      "930:\tlearn: 11.0747448\ttotal: 57m 17s\tremaining: 4m 14s\n",
      "931:\tlearn: 11.0722156\ttotal: 57m 21s\tremaining: 4m 11s\n",
      "932:\tlearn: 11.0710291\ttotal: 57m 24s\tremaining: 4m 7s\n",
      "933:\tlearn: 11.0666963\ttotal: 57m 28s\tremaining: 4m 3s\n",
      "934:\tlearn: 11.0646045\ttotal: 57m 32s\tremaining: 3m 59s\n",
      "935:\tlearn: 11.0594710\ttotal: 57m 35s\tremaining: 3m 56s\n",
      "936:\tlearn: 11.0549351\ttotal: 57m 39s\tremaining: 3m 52s\n",
      "937:\tlearn: 11.0467596\ttotal: 57m 43s\tremaining: 3m 48s\n",
      "938:\tlearn: 11.0403579\ttotal: 57m 46s\tremaining: 3m 45s\n",
      "939:\tlearn: 11.0386085\ttotal: 57m 50s\tremaining: 3m 41s\n",
      "940:\tlearn: 11.0323055\ttotal: 57m 54s\tremaining: 3m 37s\n",
      "941:\tlearn: 11.0271906\ttotal: 57m 57s\tremaining: 3m 34s\n",
      "942:\tlearn: 11.0227148\ttotal: 58m 1s\tremaining: 3m 30s\n",
      "943:\tlearn: 11.0175033\ttotal: 58m 5s\tremaining: 3m 26s\n",
      "944:\tlearn: 11.0113348\ttotal: 58m 9s\tremaining: 3m 23s\n",
      "945:\tlearn: 11.0077359\ttotal: 58m 12s\tremaining: 3m 19s\n",
      "946:\tlearn: 11.0004104\ttotal: 58m 16s\tremaining: 3m 15s\n",
      "947:\tlearn: 10.9956196\ttotal: 58m 20s\tremaining: 3m 11s\n",
      "948:\tlearn: 10.9932653\ttotal: 58m 23s\tremaining: 3m 8s\n",
      "949:\tlearn: 10.9892379\ttotal: 58m 27s\tremaining: 3m 4s\n",
      "950:\tlearn: 10.9824939\ttotal: 58m 31s\tremaining: 3m\n",
      "951:\tlearn: 10.9789346\ttotal: 58m 34s\tremaining: 2m 57s\n",
      "952:\tlearn: 10.9739217\ttotal: 58m 38s\tremaining: 2m 53s\n",
      "953:\tlearn: 10.9673181\ttotal: 58m 42s\tremaining: 2m 49s\n",
      "954:\tlearn: 10.9599332\ttotal: 58m 46s\tremaining: 2m 46s\n",
      "955:\tlearn: 10.9564817\ttotal: 58m 49s\tremaining: 2m 42s\n",
      "956:\tlearn: 10.9532236\ttotal: 58m 53s\tremaining: 2m 38s\n",
      "957:\tlearn: 10.9491888\ttotal: 58m 57s\tremaining: 2m 35s\n",
      "958:\tlearn: 10.9450992\ttotal: 59m\tremaining: 2m 31s\n",
      "959:\tlearn: 10.9390465\ttotal: 59m 4s\tremaining: 2m 27s\n",
      "960:\tlearn: 10.9360811\ttotal: 59m 8s\tremaining: 2m 23s\n",
      "961:\tlearn: 10.9315789\ttotal: 59m 11s\tremaining: 2m 20s\n",
      "962:\tlearn: 10.9286089\ttotal: 59m 15s\tremaining: 2m 16s\n",
      "963:\tlearn: 10.9230831\ttotal: 59m 19s\tremaining: 2m 12s\n",
      "964:\tlearn: 10.9190717\ttotal: 59m 22s\tremaining: 2m 9s\n",
      "965:\tlearn: 10.9123857\ttotal: 59m 26s\tremaining: 2m 5s\n",
      "966:\tlearn: 10.9086851\ttotal: 59m 30s\tremaining: 2m 1s\n",
      "967:\tlearn: 10.9029948\ttotal: 59m 33s\tremaining: 1m 58s\n",
      "968:\tlearn: 10.8989321\ttotal: 59m 37s\tremaining: 1m 54s\n",
      "969:\tlearn: 10.8927936\ttotal: 59m 41s\tremaining: 1m 50s\n",
      "970:\tlearn: 10.8882686\ttotal: 59m 44s\tremaining: 1m 47s\n",
      "971:\tlearn: 10.8849609\ttotal: 59m 48s\tremaining: 1m 43s\n",
      "972:\tlearn: 10.8805852\ttotal: 59m 52s\tremaining: 1m 39s\n",
      "973:\tlearn: 10.8735138\ttotal: 59m 56s\tremaining: 1m 35s\n",
      "974:\tlearn: 10.8658052\ttotal: 59m 59s\tremaining: 1m 32s\n",
      "975:\tlearn: 10.8609988\ttotal: 1h 3s\tremaining: 1m 28s\n",
      "976:\tlearn: 10.8590129\ttotal: 1h 7s\tremaining: 1m 24s\n",
      "977:\tlearn: 10.8563178\ttotal: 1h 10s\tremaining: 1m 21s\n",
      "978:\tlearn: 10.8503190\ttotal: 1h 14s\tremaining: 1m 17s\n",
      "979:\tlearn: 10.8474538\ttotal: 1h 18s\tremaining: 1m 13s\n",
      "980:\tlearn: 10.8428287\ttotal: 1h 21s\tremaining: 1m 10s\n",
      "981:\tlearn: 10.8376602\ttotal: 1h 25s\tremaining: 1m 6s\n",
      "982:\tlearn: 10.8330568\ttotal: 1h 29s\tremaining: 1m 2s\n",
      "983:\tlearn: 10.8283860\ttotal: 1h 32s\tremaining: 59.1s\n",
      "984:\tlearn: 10.8229840\ttotal: 1h 36s\tremaining: 55.4s\n",
      "985:\tlearn: 10.8204127\ttotal: 1h 40s\tremaining: 51.7s\n",
      "986:\tlearn: 10.8146446\ttotal: 1h 43s\tremaining: 48s\n",
      "987:\tlearn: 10.8117999\ttotal: 1h 47s\tremaining: 44.3s\n",
      "988:\tlearn: 10.8069500\ttotal: 1h 51s\tremaining: 40.6s\n",
      "989:\tlearn: 10.8028248\ttotal: 1h 54s\tremaining: 36.9s\n",
      "990:\tlearn: 10.7990646\ttotal: 1h 58s\tremaining: 33.2s\n",
      "991:\tlearn: 10.7938196\ttotal: 1h 1m 1s\tremaining: 29.5s\n",
      "992:\tlearn: 10.7906833\ttotal: 1h 1m 5s\tremaining: 25.8s\n",
      "993:\tlearn: 10.7863350\ttotal: 1h 1m 9s\tremaining: 22.1s\n",
      "994:\tlearn: 10.7823445\ttotal: 1h 1m 12s\tremaining: 18.5s\n",
      "995:\tlearn: 10.7776524\ttotal: 1h 1m 16s\tremaining: 14.8s\n",
      "996:\tlearn: 10.7744590\ttotal: 1h 1m 20s\tremaining: 11.1s\n",
      "997:\tlearn: 10.7681987\ttotal: 1h 1m 23s\tremaining: 7.38s\n",
      "998:\tlearn: 10.7627713\ttotal: 1h 1m 27s\tremaining: 3.69s\n",
      "999:\tlearn: 10.7566464\ttotal: 1h 1m 31s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# Load data into a Pool object\n",
    "train_data = Pool(X_train, y_train)\n",
    "\n",
    "# Define the model parameters\n",
    "params = {\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.1,\n",
    "    'depth': 6,\n",
    "    'loss_function': 'MultiRMSE'\n",
    "}\n",
    "\n",
    "# Create a CatBoost regressor\n",
    "multi_regressor = CatBoostRegressor(**params)\n",
    "\n",
    "# Train the model on the training data\n",
    "multi_regressor.fit(train_data)\n",
    "\n",
    "# Predict on the test data\n",
    "test_data = Pool(X_test)\n",
    "y_pred = multi_regressor.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5e1d75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:  0.9392186204390125\n",
      "mse:  2.085699547837247\n",
      "Mean Correlation Loss:  0.6972741867872163\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats  import pearsonr\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "# print(\"accuracy: \", accuracy)\n",
    "print(\"mae: \", mae)\n",
    "print(\"mse: \", mse)\n",
    "\n",
    "#accuracy for multiple columns\n",
    "\n",
    "threshold = 0.1\n",
    "corr_loss = []\n",
    "# print(y_test.shape)\n",
    "# print(type(y_test))\n",
    "# print(y_test.iloc[:10, 1])\n",
    "# print(type(mlp_y_preds))\n",
    "# print(mlp_y_preds.shape)\n",
    "# print(mlp_y_preds[:10,1])\n",
    "for i in range(y_test.shape[1]):  \n",
    "    correlation_loss_i = 1 - pearsonr(y_test.iloc[:, i], y_pred[:,i])[0]\n",
    "    corr_loss.append(correlation_loss_i)\n",
    "print(\"Mean Correlation Loss: \", np.mean(corr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a86a2754",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7dcbd774d9f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Train the MLPRegressor on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmlp_regressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Use the MLPRegressor to predict on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a MLPRegressor model\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(100,100),activation='tanh', solver='lbfgs', max_iter=1000,\n",
    "                        alpha=0.2)\n",
    "\n",
    "# Train the MLPRegressor on the training data\n",
    "mlp_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Use the MLPRegressor to predict on the test data\n",
    "y_pred_mlp = mlp_regressor.predict(X_test)\n",
    "\n",
    "# Build a CatBoostRegressor model\n",
    "catboost_regressor = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='MultiRMSE', random_state=42)\n",
    "catboost_data = Pool(X_train, y_train)\n",
    "# Train the CatBoostRegressor on the training data\n",
    "catboost_regressor.fit(catboost_data)\n",
    "\n",
    "# Use the CatBoostRegressor to predict on the test data\n",
    "catboost_X_test = Pool(X_test)\n",
    "y_pred_catboost = catboost_regressor.predict(X_test)\n",
    "\n",
    "# Concatenate the predicted outputs from both models\n",
    "concatenated_preds = np.concatenate((y_pred_mlp, y_pred_catboost), axis=1)\n",
    "\n",
    "# Build a new MLPRegressor model to combine the predictions\n",
    "combined_regressor = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the combined MLPRegressor model on the concatenated predictions\n",
    "combined_regressor.fit(concatenated_preds, y_test)\n",
    "\n",
    "# Use the MLPRegressor to predict on the test data\n",
    "y_pred_combined = combined_regressor.predict(concatenated_preds)\n",
    "\n",
    "# Calculate the mean squared error for the combined MLPRegressor\n",
    "mse_combined = mean_squared_error(y_test, y_pred_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25406ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7581193192038813\n"
     ]
    }
   ],
   "source": [
    "print(mse_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da435531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000121410_A1BG</th>\n",
       "      <th>ENSG00000268895_A1BG-AS1</th>\n",
       "      <th>ENSG00000175899_A2M</th>\n",
       "      <th>ENSG00000245105_A2M-AS1</th>\n",
       "      <th>ENSG00000166535_A2ML1</th>\n",
       "      <th>ENSG00000128274_A4GALT</th>\n",
       "      <th>ENSG00000094914_AAAS</th>\n",
       "      <th>ENSG00000081760_AACS</th>\n",
       "      <th>ENSG00000109576_AADAT</th>\n",
       "      <th>ENSG00000103591_AAGAB</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000153975_ZUP1</th>\n",
       "      <th>ENSG00000086827_ZW10</th>\n",
       "      <th>ENSG00000174442_ZWILCH</th>\n",
       "      <th>ENSG00000122952_ZWINT</th>\n",
       "      <th>ENSG00000198205_ZXDA</th>\n",
       "      <th>ENSG00000198455_ZXDB</th>\n",
       "      <th>ENSG00000070476_ZXDC</th>\n",
       "      <th>ENSG00000162378_ZYG11B</th>\n",
       "      <th>ENSG00000159840_ZYX</th>\n",
       "      <th>ENSG00000074755_ZZEF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000141b198e6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.271958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003fcc59086</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.842476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.842476</td>\n",
       "      <td>1.252469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000441bba05a</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004897a2b2b</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000517abfb58</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00056fb385f2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005b45cd15b</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.104063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.358000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.82448</td>\n",
       "      <td>0.82448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00097bbec9a2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.838487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.838487</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0009a974f266</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.838241</td>\n",
       "      <td>0.838241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.838241</td>\n",
       "      <td>1.095511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.838241</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000a9f1e0b14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.266184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22050 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ENSG00000121410_A1BG  ENSG00000268895_A1BG-AS1  \\\n",
       "000141b198e6                   0.0                       0.0   \n",
       "0003fcc59086                   0.0                       0.0   \n",
       "000441bba05a                   0.0                       0.0   \n",
       "0004897a2b2b                   0.0                       0.0   \n",
       "000517abfb58                   0.0                       0.0   \n",
       "00056fb385f2                   0.0                       0.0   \n",
       "0005b45cd15b                   0.0                       0.0   \n",
       "00097bbec9a2                   0.0                       0.0   \n",
       "0009a974f266                   0.0                       0.0   \n",
       "000a9f1e0b14                   0.0                       1.0   \n",
       "\n",
       "              ENSG00000175899_A2M  ENSG00000245105_A2M-AS1  \\\n",
       "000141b198e6                  0.0                      0.0   \n",
       "0003fcc59086                  0.0                      0.0   \n",
       "000441bba05a                  0.0                      0.0   \n",
       "0004897a2b2b                  0.0                      0.0   \n",
       "000517abfb58                  0.0                      0.0   \n",
       "00056fb385f2                  0.0                      0.0   \n",
       "0005b45cd15b                  0.0                      0.0   \n",
       "00097bbec9a2                  0.0                      0.0   \n",
       "0009a974f266                  0.0                      0.0   \n",
       "000a9f1e0b14                  0.0                      0.0   \n",
       "\n",
       "              ENSG00000166535_A2ML1  ENSG00000128274_A4GALT  \\\n",
       "000141b198e6                    0.0                     0.0   \n",
       "0003fcc59086                    0.0                     0.0   \n",
       "000441bba05a                    0.0                     0.0   \n",
       "0004897a2b2b                    0.0                     0.0   \n",
       "000517abfb58                    0.0                     0.0   \n",
       "00056fb385f2                    0.0                     0.0   \n",
       "0005b45cd15b                    0.0                     0.0   \n",
       "00097bbec9a2                    0.0                     0.0   \n",
       "0009a974f266                    0.0                     0.0   \n",
       "000a9f1e0b14                    0.0                     0.0   \n",
       "\n",
       "              ENSG00000094914_AAAS  ENSG00000081760_AACS  \\\n",
       "000141b198e6              0.000000              1.000000   \n",
       "0003fcc59086              0.000000              0.000000   \n",
       "000441bba05a              0.000000              0.000000   \n",
       "0004897a2b2b              0.000000              0.000000   \n",
       "000517abfb58              0.000000              1.000000   \n",
       "00056fb385f2              1.000000              0.000000   \n",
       "0005b45cd15b              1.104063              1.000000   \n",
       "00097bbec9a2              1.000000              0.000000   \n",
       "0009a974f266              0.838241              0.838241   \n",
       "000a9f1e0b14              0.000000              0.000000   \n",
       "\n",
       "              ENSG00000109576_AADAT  ENSG00000103591_AAGAB  ...  \\\n",
       "000141b198e6                    0.0               0.000000  ...   \n",
       "0003fcc59086                    0.0               0.842476  ...   \n",
       "000441bba05a                    0.0               0.000000  ...   \n",
       "0004897a2b2b                    0.0               0.000000  ...   \n",
       "000517abfb58                    0.0               0.000000  ...   \n",
       "00056fb385f2                    0.0               0.000000  ...   \n",
       "0005b45cd15b                    0.0               0.000000  ...   \n",
       "00097bbec9a2                    0.0               0.838487  ...   \n",
       "0009a974f266                    0.0               0.000000  ...   \n",
       "000a9f1e0b14                    0.0               0.000000  ...   \n",
       "\n",
       "              ENSG00000153975_ZUP1  ENSG00000086827_ZW10  \\\n",
       "000141b198e6                   0.0                   0.0   \n",
       "0003fcc59086                   0.0                   1.0   \n",
       "000441bba05a                   1.0                   0.0   \n",
       "0004897a2b2b                   0.0                   0.0   \n",
       "000517abfb58                   0.0                   0.0   \n",
       "00056fb385f2                   0.0                   0.0   \n",
       "0005b45cd15b                   0.0                   0.0   \n",
       "00097bbec9a2                   0.0                   1.0   \n",
       "0009a974f266                   1.0                   1.0   \n",
       "000a9f1e0b14                   1.0                   0.0   \n",
       "\n",
       "              ENSG00000174442_ZWILCH  ENSG00000122952_ZWINT  \\\n",
       "000141b198e6                0.000000               1.271958   \n",
       "0003fcc59086                0.842476               1.252469   \n",
       "000441bba05a                0.000000               0.000000   \n",
       "0004897a2b2b                0.000000               0.000000   \n",
       "000517abfb58                1.000000               0.000000   \n",
       "00056fb385f2                0.000000               0.000000   \n",
       "0005b45cd15b                1.000000               1.358000   \n",
       "00097bbec9a2                0.000000               0.838487   \n",
       "0009a974f266                0.838241               1.095511   \n",
       "000a9f1e0b14                0.000000               1.266184   \n",
       "\n",
       "              ENSG00000198205_ZXDA  ENSG00000198455_ZXDB  \\\n",
       "000141b198e6                   0.0              0.000000   \n",
       "0003fcc59086                   0.0              0.000000   \n",
       "000441bba05a                   0.0              0.000000   \n",
       "0004897a2b2b                   0.0              0.000000   \n",
       "000517abfb58                   0.0              0.000000   \n",
       "00056fb385f2                   0.0              0.000000   \n",
       "0005b45cd15b                   0.0              0.000000   \n",
       "00097bbec9a2                   0.0              0.838487   \n",
       "0009a974f266                   0.0              0.000000   \n",
       "000a9f1e0b14                   0.0              0.000000   \n",
       "\n",
       "              ENSG00000070476_ZXDC  ENSG00000162378_ZYG11B  \\\n",
       "000141b198e6               0.00000                 0.00000   \n",
       "0003fcc59086               0.00000                 0.00000   \n",
       "000441bba05a               0.00000                 0.00000   \n",
       "0004897a2b2b               0.00000                 0.00000   \n",
       "000517abfb58               0.00000                 1.00000   \n",
       "00056fb385f2               1.00000                 0.00000   \n",
       "0005b45cd15b               0.82448                 0.82448   \n",
       "00097bbec9a2               0.00000                 0.00000   \n",
       "0009a974f266               0.00000                 0.00000   \n",
       "000a9f1e0b14               0.00000                 0.00000   \n",
       "\n",
       "              ENSG00000159840_ZYX  ENSG00000074755_ZZEF1  \n",
       "000141b198e6             0.000000                    0.0  \n",
       "0003fcc59086             0.000000                    0.0  \n",
       "000441bba05a             0.000000                    0.0  \n",
       "0004897a2b2b             1.000000                    0.0  \n",
       "000517abfb58             1.000000                    0.0  \n",
       "00056fb385f2             1.000000                    0.0  \n",
       "0005b45cd15b             1.000000                    0.0  \n",
       "00097bbec9a2             1.000000                    0.0  \n",
       "0009a974f266             0.838241                    0.0  \n",
       "000a9f1e0b14             0.000000                    0.0  \n",
       "\n",
       "[10 rows x 22050 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate and divide by nonzero median\n",
    "row_median = lambda row: np.median(row[row != 0])\n",
    "nonzero_medians = df_train_input.apply(row_median,axis=1) \n",
    "df_div= df_train_input.div(nonzero_medians,axis=0) \n",
    "df_div.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a665c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000121410_A1BG</th>\n",
       "      <th>ENSG00000268895_A1BG-AS1</th>\n",
       "      <th>ENSG00000175899_A2M</th>\n",
       "      <th>ENSG00000245105_A2M-AS1</th>\n",
       "      <th>ENSG00000166535_A2ML1</th>\n",
       "      <th>ENSG00000128274_A4GALT</th>\n",
       "      <th>ENSG00000094914_AAAS</th>\n",
       "      <th>ENSG00000081760_AACS</th>\n",
       "      <th>ENSG00000109576_AADAT</th>\n",
       "      <th>ENSG00000103591_AAGAB</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000153975_ZUP1</th>\n",
       "      <th>ENSG00000086827_ZW10</th>\n",
       "      <th>ENSG00000174442_ZWILCH</th>\n",
       "      <th>ENSG00000122952_ZWINT</th>\n",
       "      <th>ENSG00000198205_ZXDA</th>\n",
       "      <th>ENSG00000198455_ZXDB</th>\n",
       "      <th>ENSG00000070476_ZXDC</th>\n",
       "      <th>ENSG00000162378_ZYG11B</th>\n",
       "      <th>ENSG00000159840_ZYX</th>\n",
       "      <th>ENSG00000074755_ZZEF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000141b198e6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003fcc59086</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.611110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.611110</td>\n",
       "      <td>0.812027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000441bba05a</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004897a2b2b</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000517abfb58</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00056fb385f2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005b45cd15b</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.743870</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.857814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.601295</td>\n",
       "      <td>0.601295</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00097bbec9a2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.608943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.608943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.608943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0009a974f266</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.608809</td>\n",
       "      <td>0.608809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.608809</td>\n",
       "      <td>0.739797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.608809</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000a9f1e0b14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.818097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22050 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ENSG00000121410_A1BG  ENSG00000268895_A1BG-AS1  \\\n",
       "000141b198e6                   0.0                  0.000000   \n",
       "0003fcc59086                   0.0                  0.000000   \n",
       "000441bba05a                   0.0                  0.000000   \n",
       "0004897a2b2b                   0.0                  0.000000   \n",
       "000517abfb58                   0.0                  0.000000   \n",
       "00056fb385f2                   0.0                  0.000000   \n",
       "0005b45cd15b                   0.0                  0.000000   \n",
       "00097bbec9a2                   0.0                  0.000000   \n",
       "0009a974f266                   0.0                  0.000000   \n",
       "000a9f1e0b14                   0.0                  0.693147   \n",
       "\n",
       "              ENSG00000175899_A2M  ENSG00000245105_A2M-AS1  \\\n",
       "000141b198e6                  0.0                      0.0   \n",
       "0003fcc59086                  0.0                      0.0   \n",
       "000441bba05a                  0.0                      0.0   \n",
       "0004897a2b2b                  0.0                      0.0   \n",
       "000517abfb58                  0.0                      0.0   \n",
       "00056fb385f2                  0.0                      0.0   \n",
       "0005b45cd15b                  0.0                      0.0   \n",
       "00097bbec9a2                  0.0                      0.0   \n",
       "0009a974f266                  0.0                      0.0   \n",
       "000a9f1e0b14                  0.0                      0.0   \n",
       "\n",
       "              ENSG00000166535_A2ML1  ENSG00000128274_A4GALT  \\\n",
       "000141b198e6                    0.0                     0.0   \n",
       "0003fcc59086                    0.0                     0.0   \n",
       "000441bba05a                    0.0                     0.0   \n",
       "0004897a2b2b                    0.0                     0.0   \n",
       "000517abfb58                    0.0                     0.0   \n",
       "00056fb385f2                    0.0                     0.0   \n",
       "0005b45cd15b                    0.0                     0.0   \n",
       "00097bbec9a2                    0.0                     0.0   \n",
       "0009a974f266                    0.0                     0.0   \n",
       "000a9f1e0b14                    0.0                     0.0   \n",
       "\n",
       "              ENSG00000094914_AAAS  ENSG00000081760_AACS  \\\n",
       "000141b198e6              0.000000              0.693147   \n",
       "0003fcc59086              0.000000              0.000000   \n",
       "000441bba05a              0.000000              0.000000   \n",
       "0004897a2b2b              0.000000              0.000000   \n",
       "000517abfb58              0.000000              0.693147   \n",
       "00056fb385f2              0.693147              0.000000   \n",
       "0005b45cd15b              0.743870              0.693147   \n",
       "00097bbec9a2              0.693147              0.000000   \n",
       "0009a974f266              0.608809              0.608809   \n",
       "000a9f1e0b14              0.000000              0.000000   \n",
       "\n",
       "              ENSG00000109576_AADAT  ENSG00000103591_AAGAB  ...  \\\n",
       "000141b198e6                    0.0               0.000000  ...   \n",
       "0003fcc59086                    0.0               0.611110  ...   \n",
       "000441bba05a                    0.0               0.000000  ...   \n",
       "0004897a2b2b                    0.0               0.000000  ...   \n",
       "000517abfb58                    0.0               0.000000  ...   \n",
       "00056fb385f2                    0.0               0.000000  ...   \n",
       "0005b45cd15b                    0.0               0.000000  ...   \n",
       "00097bbec9a2                    0.0               0.608943  ...   \n",
       "0009a974f266                    0.0               0.000000  ...   \n",
       "000a9f1e0b14                    0.0               0.000000  ...   \n",
       "\n",
       "              ENSG00000153975_ZUP1  ENSG00000086827_ZW10  \\\n",
       "000141b198e6              0.000000              0.000000   \n",
       "0003fcc59086              0.000000              0.693147   \n",
       "000441bba05a              0.693147              0.000000   \n",
       "0004897a2b2b              0.000000              0.000000   \n",
       "000517abfb58              0.000000              0.000000   \n",
       "00056fb385f2              0.000000              0.000000   \n",
       "0005b45cd15b              0.000000              0.000000   \n",
       "00097bbec9a2              0.000000              0.693147   \n",
       "0009a974f266              0.693147              0.693147   \n",
       "000a9f1e0b14              0.693147              0.000000   \n",
       "\n",
       "              ENSG00000174442_ZWILCH  ENSG00000122952_ZWINT  \\\n",
       "000141b198e6                0.000000               0.820642   \n",
       "0003fcc59086                0.611110               0.812027   \n",
       "000441bba05a                0.000000               0.000000   \n",
       "0004897a2b2b                0.000000               0.000000   \n",
       "000517abfb58                0.693147               0.000000   \n",
       "00056fb385f2                0.000000               0.000000   \n",
       "0005b45cd15b                0.693147               0.857814   \n",
       "00097bbec9a2                0.000000               0.608943   \n",
       "0009a974f266                0.608809               0.739797   \n",
       "000a9f1e0b14                0.000000               0.818097   \n",
       "\n",
       "              ENSG00000198205_ZXDA  ENSG00000198455_ZXDB  \\\n",
       "000141b198e6                   0.0              0.000000   \n",
       "0003fcc59086                   0.0              0.000000   \n",
       "000441bba05a                   0.0              0.000000   \n",
       "0004897a2b2b                   0.0              0.000000   \n",
       "000517abfb58                   0.0              0.000000   \n",
       "00056fb385f2                   0.0              0.000000   \n",
       "0005b45cd15b                   0.0              0.000000   \n",
       "00097bbec9a2                   0.0              0.608943   \n",
       "0009a974f266                   0.0              0.000000   \n",
       "000a9f1e0b14                   0.0              0.000000   \n",
       "\n",
       "              ENSG00000070476_ZXDC  ENSG00000162378_ZYG11B  \\\n",
       "000141b198e6              0.000000                0.000000   \n",
       "0003fcc59086              0.000000                0.000000   \n",
       "000441bba05a              0.000000                0.000000   \n",
       "0004897a2b2b              0.000000                0.000000   \n",
       "000517abfb58              0.000000                0.693147   \n",
       "00056fb385f2              0.693147                0.000000   \n",
       "0005b45cd15b              0.601295                0.601295   \n",
       "00097bbec9a2              0.000000                0.000000   \n",
       "0009a974f266              0.000000                0.000000   \n",
       "000a9f1e0b14              0.000000                0.000000   \n",
       "\n",
       "              ENSG00000159840_ZYX  ENSG00000074755_ZZEF1  \n",
       "000141b198e6             0.000000                    0.0  \n",
       "0003fcc59086             0.000000                    0.0  \n",
       "000441bba05a             0.000000                    0.0  \n",
       "0004897a2b2b             0.693147                    0.0  \n",
       "000517abfb58             0.693147                    0.0  \n",
       "00056fb385f2             0.693147                    0.0  \n",
       "0005b45cd15b             0.693147                    0.0  \n",
       "00097bbec9a2             0.693147                    0.0  \n",
       "0009a974f266             0.608809                    0.0  \n",
       "000a9f1e0b14             0.000000                    0.0  \n",
       "\n",
       "[10 rows x 22050 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform with log1p\n",
    "df_log = df_div.applymap(np.log1p)\n",
    "df_log.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dd4e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_input = df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92841aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000121410_A1BG</th>\n",
       "      <th>ENSG00000268895_A1BG-AS1</th>\n",
       "      <th>ENSG00000175899_A2M</th>\n",
       "      <th>ENSG00000245105_A2M-AS1</th>\n",
       "      <th>ENSG00000166535_A2ML1</th>\n",
       "      <th>ENSG00000128274_A4GALT</th>\n",
       "      <th>ENSG00000094914_AAAS</th>\n",
       "      <th>ENSG00000081760_AACS</th>\n",
       "      <th>ENSG00000109576_AADAT</th>\n",
       "      <th>ENSG00000103591_AAGAB</th>\n",
       "      <th>...</th>\n",
       "      <th>CD94</th>\n",
       "      <th>CD162</th>\n",
       "      <th>CD85j</th>\n",
       "      <th>CD23</th>\n",
       "      <th>CD328</th>\n",
       "      <th>HLA-E</th>\n",
       "      <th>CD82</th>\n",
       "      <th>CD101</th>\n",
       "      <th>CD88</th>\n",
       "      <th>CD224</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000141b198e6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144601</td>\n",
       "      <td>10.489143</td>\n",
       "      <td>0.939881</td>\n",
       "      <td>0.841505</td>\n",
       "      <td>3.309048</td>\n",
       "      <td>1.978804</td>\n",
       "      <td>13.027639</td>\n",
       "      <td>-0.208923</td>\n",
       "      <td>2.542908</td>\n",
       "      <td>8.071543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003fcc59086</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.61111</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.595230</td>\n",
       "      <td>6.023785</td>\n",
       "      <td>0.408263</td>\n",
       "      <td>-0.497212</td>\n",
       "      <td>-0.133765</td>\n",
       "      <td>2.195598</td>\n",
       "      <td>9.705816</td>\n",
       "      <td>-0.580132</td>\n",
       "      <td>7.645536</td>\n",
       "      <td>5.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000441bba05a</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814176</td>\n",
       "      <td>9.285316</td>\n",
       "      <td>0.299309</td>\n",
       "      <td>1.803804</td>\n",
       "      <td>0.051648</td>\n",
       "      <td>1.212216</td>\n",
       "      <td>3.949074</td>\n",
       "      <td>1.440157</td>\n",
       "      <td>3.350088</td>\n",
       "      <td>0.584580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004897a2b2b</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.829338</td>\n",
       "      <td>5.624609</td>\n",
       "      <td>2.015439</td>\n",
       "      <td>4.492601</td>\n",
       "      <td>-1.151961</td>\n",
       "      <td>3.715674</td>\n",
       "      <td>5.097805</td>\n",
       "      <td>1.668119</td>\n",
       "      <td>1.643337</td>\n",
       "      <td>5.632807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000517abfb58</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.066876</td>\n",
       "      <td>7.712044</td>\n",
       "      <td>-0.332271</td>\n",
       "      <td>-0.608914</td>\n",
       "      <td>2.938849</td>\n",
       "      <td>0.370199</td>\n",
       "      <td>7.530865</td>\n",
       "      <td>-0.520287</td>\n",
       "      <td>2.078689</td>\n",
       "      <td>2.115581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0054b21867c8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473038</td>\n",
       "      <td>2.826349</td>\n",
       "      <td>0.116303</td>\n",
       "      <td>0.640916</td>\n",
       "      <td>-0.020092</td>\n",
       "      <td>-0.048039</td>\n",
       "      <td>3.671904</td>\n",
       "      <td>1.891259</td>\n",
       "      <td>3.929412</td>\n",
       "      <td>2.359597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>005569028ddd</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148417</td>\n",
       "      <td>10.783627</td>\n",
       "      <td>1.109807</td>\n",
       "      <td>0.961539</td>\n",
       "      <td>0.070386</td>\n",
       "      <td>1.774466</td>\n",
       "      <td>4.031822</td>\n",
       "      <td>0.486662</td>\n",
       "      <td>3.392267</td>\n",
       "      <td>5.753120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>005633f1ea16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.611884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593003</td>\n",
       "      <td>6.540831</td>\n",
       "      <td>2.188413</td>\n",
       "      <td>0.337904</td>\n",
       "      <td>-0.449624</td>\n",
       "      <td>-0.181593</td>\n",
       "      <td>5.820107</td>\n",
       "      <td>-0.139723</td>\n",
       "      <td>5.907623</td>\n",
       "      <td>2.739270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0057cff787fe</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.366140</td>\n",
       "      <td>0.343975</td>\n",
       "      <td>0.556323</td>\n",
       "      <td>-1.113267</td>\n",
       "      <td>0.377743</td>\n",
       "      <td>-0.448942</td>\n",
       "      <td>0.539068</td>\n",
       "      <td>0.497742</td>\n",
       "      <td>2.360866</td>\n",
       "      <td>0.351199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00587ef7e9af</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.276978</td>\n",
       "      <td>12.201834</td>\n",
       "      <td>-1.172982</td>\n",
       "      <td>-0.368008</td>\n",
       "      <td>-0.267918</td>\n",
       "      <td>1.004077</td>\n",
       "      <td>7.144591</td>\n",
       "      <td>1.973109</td>\n",
       "      <td>8.134394</td>\n",
       "      <td>2.750627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 22190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ENSG00000121410_A1BG  ENSG00000268895_A1BG-AS1  \\\n",
       "000141b198e6                   0.0                       0.0   \n",
       "0003fcc59086                   0.0                       0.0   \n",
       "000441bba05a                   0.0                       0.0   \n",
       "0004897a2b2b                   0.0                       0.0   \n",
       "000517abfb58                   0.0                       0.0   \n",
       "...                            ...                       ...   \n",
       "0054b21867c8                   0.0                       0.0   \n",
       "005569028ddd                   0.0                       0.0   \n",
       "005633f1ea16                   0.0                       0.0   \n",
       "0057cff787fe                   0.0                       0.0   \n",
       "00587ef7e9af                   0.0                       0.0   \n",
       "\n",
       "              ENSG00000175899_A2M  ENSG00000245105_A2M-AS1  \\\n",
       "000141b198e6                  0.0                 0.000000   \n",
       "0003fcc59086                  0.0                 0.000000   \n",
       "000441bba05a                  0.0                 0.000000   \n",
       "0004897a2b2b                  0.0                 0.000000   \n",
       "000517abfb58                  0.0                 0.000000   \n",
       "...                           ...                      ...   \n",
       "0054b21867c8                  0.0                 0.000000   \n",
       "005569028ddd                  0.0                 0.693147   \n",
       "005633f1ea16                  0.0                 0.000000   \n",
       "0057cff787fe                  0.0                 0.000000   \n",
       "00587ef7e9af                  0.0                 0.000000   \n",
       "\n",
       "              ENSG00000166535_A2ML1  ENSG00000128274_A4GALT  \\\n",
       "000141b198e6                    0.0                     0.0   \n",
       "0003fcc59086                    0.0                     0.0   \n",
       "000441bba05a                    0.0                     0.0   \n",
       "0004897a2b2b                    0.0                     0.0   \n",
       "000517abfb58                    0.0                     0.0   \n",
       "...                             ...                     ...   \n",
       "0054b21867c8                    0.0                     0.0   \n",
       "005569028ddd                    0.0                     0.0   \n",
       "005633f1ea16                    0.0                     0.0   \n",
       "0057cff787fe                    0.0                     0.0   \n",
       "00587ef7e9af                    0.0                     0.0   \n",
       "\n",
       "              ENSG00000094914_AAAS  ENSG00000081760_AACS  \\\n",
       "000141b198e6              0.000000              0.693147   \n",
       "0003fcc59086              0.000000              0.000000   \n",
       "000441bba05a              0.000000              0.000000   \n",
       "0004897a2b2b              0.000000              0.000000   \n",
       "000517abfb58              0.000000              0.693147   \n",
       "...                            ...                   ...   \n",
       "0054b21867c8              0.000000              0.000000   \n",
       "005569028ddd              0.693147              0.000000   \n",
       "005633f1ea16              0.611884              0.000000   \n",
       "0057cff787fe              0.000000              0.000000   \n",
       "00587ef7e9af              0.000000              0.000000   \n",
       "\n",
       "              ENSG00000109576_AADAT  ENSG00000103591_AAGAB  ...      CD94  \\\n",
       "000141b198e6                    0.0                0.00000  ...  0.144601   \n",
       "0003fcc59086                    0.0                0.61111  ... -0.595230   \n",
       "000441bba05a                    0.0                0.00000  ...  0.814176   \n",
       "0004897a2b2b                    0.0                0.00000  ...  4.829338   \n",
       "000517abfb58                    0.0                0.00000  ... -1.066876   \n",
       "...                             ...                    ...  ...       ...   \n",
       "0054b21867c8                    0.0                0.00000  ...  0.473038   \n",
       "005569028ddd                    0.0                0.00000  ... -0.148417   \n",
       "005633f1ea16                    0.0                0.00000  ...  0.593003   \n",
       "0057cff787fe                    0.0                0.00000  ... -0.366140   \n",
       "00587ef7e9af                    0.0                0.00000  ...  1.276978   \n",
       "\n",
       "                  CD162     CD85j      CD23     CD328     HLA-E       CD82  \\\n",
       "000141b198e6  10.489143  0.939881  0.841505  3.309048  1.978804  13.027639   \n",
       "0003fcc59086   6.023785  0.408263 -0.497212 -0.133765  2.195598   9.705816   \n",
       "000441bba05a   9.285316  0.299309  1.803804  0.051648  1.212216   3.949074   \n",
       "0004897a2b2b   5.624609  2.015439  4.492601 -1.151961  3.715674   5.097805   \n",
       "000517abfb58   7.712044 -0.332271 -0.608914  2.938849  0.370199   7.530865   \n",
       "...                 ...       ...       ...       ...       ...        ...   \n",
       "0054b21867c8   2.826349  0.116303  0.640916 -0.020092 -0.048039   3.671904   \n",
       "005569028ddd  10.783627  1.109807  0.961539  0.070386  1.774466   4.031822   \n",
       "005633f1ea16   6.540831  2.188413  0.337904 -0.449624 -0.181593   5.820107   \n",
       "0057cff787fe   0.343975  0.556323 -1.113267  0.377743 -0.448942   0.539068   \n",
       "00587ef7e9af  12.201834 -1.172982 -0.368008 -0.267918  1.004077   7.144591   \n",
       "\n",
       "                 CD101      CD88     CD224  \n",
       "000141b198e6 -0.208923  2.542908  8.071543  \n",
       "0003fcc59086 -0.580132  7.645536  5.570200  \n",
       "000441bba05a  1.440157  3.350088  0.584580  \n",
       "0004897a2b2b  1.668119  1.643337  5.632807  \n",
       "000517abfb58 -0.520287  2.078689  2.115581  \n",
       "...                ...       ...       ...  \n",
       "0054b21867c8  1.891259  3.929412  2.359597  \n",
       "005569028ddd  0.486662  3.392267  5.753120  \n",
       "005633f1ea16 -0.139723  5.907623  2.739270  \n",
       "0057cff787fe  0.497742  2.360866  0.351199  \n",
       "00587ef7e9af  1.973109  8.134394  2.750627  \n",
       "\n",
       "[100 rows x 22190 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inner join\n",
    "# df_merged = df_train_input.merge(df_train_target,how='inner')#,lsuffix='l_',rsuffix='r_')\n",
    "df_merged = pd.merge(df_train_input,df_train_target,left_index=True, right_index=True)\n",
    "df_merged.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83da4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate corr coef\n",
    "target_columns = \"','\".join(list(axis0_arr))\n",
    "\n",
    "# corr_df = df_merged.corr()[target_columns]\n",
    "# corr_threshold = .42\n",
    "# hcr_df = df_merged.loc[(corr_df.abs() > corr_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dd6dff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head()\n",
    "len(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e900483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CD86' 'CD274' 'CD270' 'CD155' 'CD112' 'CD47' 'CD48' 'CD40' 'CD154'\n",
      " 'CD52' 'CD3' 'CD8' 'CD56' 'CD19' 'CD33' 'CD11c' 'HLA-A-B-C' 'CD45RA'\n",
      " 'CD123' 'CD7' 'CD105' 'CD49f' 'CD194' 'CD4' 'CD44' 'CD14' 'CD16' 'CD25'\n",
      " 'CD45RO' 'CD279' 'TIGIT' 'Mouse-IgG1' 'Mouse-IgG2a' 'Mouse-IgG2b'\n",
      " 'Rat-IgG2b' 'CD20' 'CD335' 'CD31' 'Podoplanin' 'CD146' 'IgM' 'CD5'\n",
      " 'CD195' 'CD32' 'CD196' 'CD185' 'CD103' 'CD69' 'CD62L' 'CD161' 'CD152'\n",
      " 'CD223' 'KLRG1' 'CD27' 'CD107a' 'CD95' 'CD134' 'HLA-DR' 'CD1c' 'CD11b'\n",
      " 'CD64' 'CD141' 'CD1d' 'CD314' 'CD35' 'CD57' 'CD272' 'CD278' 'CD58' 'CD39'\n",
      " 'CX3CR1' 'CD24' 'CD21' 'CD11a' 'CD79b' 'CD244' 'CD169' 'integrinB7'\n",
      " 'CD268' 'CD42b' 'CD54' 'CD62P' 'CD119' 'TCR' 'Rat-IgG1' 'Rat-IgG2a'\n",
      " 'CD192' 'CD122' 'FceRIa' 'CD41' 'CD137' 'CD163' 'CD83' 'CD124' 'CD13'\n",
      " 'CD2' 'CD226' 'CD29' 'CD303' 'CD49b' 'CD81' 'IgD' 'CD18' 'CD28' 'CD38'\n",
      " 'CD127' 'CD45' 'CD22' 'CD71' 'CD26' 'CD115' 'CD63' 'CD304' 'CD36'\n",
      " 'CD172a' 'CD72' 'CD158' 'CD93' 'CD49a' 'CD49d' 'CD73' 'CD9' 'TCRVa7.2'\n",
      " 'TCRVd2' 'LOX-1' 'CD158b' 'CD158e1' 'CD142' 'CD319' 'CD352' 'CD94'\n",
      " 'CD162' 'CD85j' 'CD23' 'CD328' 'HLA-E' 'CD82' 'CD101' 'CD88' 'CD224']\n",
      "[[ 4.17697867  1.36461641  2.5550035  ...  4.23919503  2.44229826\n",
      "   5.06487398]\n",
      " [ 0.86448771  1.01392888  1.74647902 ...  0.8942968   7.04314538\n",
      "   7.17315707]\n",
      " [ 0.29955621  0.7856592   1.1738546  ...  0.42145799  3.54574145\n",
      "   1.33808466]\n",
      " ...\n",
      " [-0.35626742  0.28186387  0.72771074 ... -0.0715049   6.43546915\n",
      "   2.56433602]\n",
      " [ 0.49267219  0.34027236 -0.13801273 ...  0.28127745  1.56810838\n",
      "   0.35246911]\n",
      " [ 0.0292484   0.4832315   1.15159724 ...  0.18743451  7.63765373\n",
      "   4.21102722]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "target_columns = df_train_target.columns.values\n",
    "print(target_columns)\n",
    "# Separate the input and target columns\n",
    "X = df_merged.drop(target_columns, axis=1)\n",
    "y = df_merged[target_columns]\n",
    "\n",
    "# Initialize the MLPRegressor and CatBoostRegressor models\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500)\n",
    "# catboost_model = CatBoostRegressor(iterations=1000, learning_rate=0.1)\n",
    "\n",
    "# Train the models on the dataset\n",
    "mlp_model.fit(X, y)\n",
    "# catboost_model.fit(X, y)\n",
    "\n",
    "# Predict the target values using the trained models\n",
    "mlp_preds = mlp_model.predict(X)\n",
    "# catboost_preds = catboost_model.predict(X)\n",
    "\n",
    "# mlp1_score = mlp_model.score(test_features, test_target1)\n",
    "# mlp2_score = mlp2.score(test_features, test_target2)\n",
    "\n",
    "print(mlp_preds[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adcdda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numero uno leaderboard preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_norm = StandardScaler().fit_transform(df_train_input.values)\n",
    "df_median = np.median(df_norm, axis=0)\n",
    "df_norm_median = df_norm - df_median\n",
    "\n",
    "tsvd = TruncatedSVD(n_components=128)\n",
    "df_tsvd = tsvd.fit_transform(df_norm_median)\n",
    "\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
